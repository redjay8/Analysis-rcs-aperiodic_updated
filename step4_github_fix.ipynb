{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f43b35-ba44-4cb5-b9e1-a8afc93dd13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 1: Centralized Imports and Global Configuration ---\n",
    "\n",
    "# --- Part 1: All Library Imports ---\n",
    "# Python Core Libraries\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime, time\n",
    "\n",
    "# Data Handling & Scientific Computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from scipy.stats import (pearsonr, spearmanr, mannwhitneyu, t, kruskal)\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Statistical Modeling\n",
    "import pingouin as pg\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import plot_partregress_grid\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "# Plotting & Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Degrees of freedom <= 0 for slice\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"p-value may not be accurate for N > 5000\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in scalar divide\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Confidence interval might not be reliable for bootstrap samples with fewer than 50 elements.\")\n",
    "\n",
    "# --- Part 3: User Input and Path Configuration ---\n",
    "# This script processes ONE patient-hemisphere at a time.\n",
    "patient_hemisphere_id = \"RCS20R\" # <<< USER: SET THIS FOR THE CURRENT PATIENT-HEMISPHERE\n",
    "project_base_path = '..'\n",
    "step3_output_version_tag = \"neural_pkg_aligned\" # <<< USER: Ensure this matches Step 3's tag\n",
    "\n",
    "# Derived Paths (no user input needed below this line)\n",
    "step3_master_csv_base_folder = os.path.join(project_base_path, f'step3_fooof_results_{step3_output_version_tag}')\n",
    "master_csv_filename = f\"MASTER_FOOOF_PKG_results_{patient_hemisphere_id}_{step3_output_version_tag}.csv\"\n",
    "master_csv_path_to_load = os.path.join(step3_master_csv_base_folder, master_csv_filename)\n",
    "step4_analysis_root_folder = os.path.join(step3_master_csv_base_folder, 'step4_within_subject')\n",
    "os.makedirs(step4_analysis_root_folder, exist_ok=True)\n",
    "current_datetime_str_step4 = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "session_plot_folder_name_step4 = f\"{patient_hemisphere_id}_plots_{current_datetime_str_step4}\"\n",
    "analysis_session_plot_folder_step4 = os.path.join(step4_analysis_root_folder, session_plot_folder_name_step4)\n",
    "os.makedirs(analysis_session_plot_folder_step4, exist_ok=True)\n",
    "\n",
    "# --- Part 4: Column Name and Metric Definitions ---\n",
    "# Metric Column Dictionaries\n",
    "APERIODIC_METRICS_COLS = {\n",
    "    'Exponent_BestModel': 'Aperiodic Exponent',\n",
    "    'Offset_BestModel': 'Aperiodic Offset',\n",
    "}\n",
    "PKG_METRICS_COLS = {\n",
    "    'Aligned_BK': 'PKG BK Score',\n",
    "    'Aligned_DK': 'PKG DK Score',\n",
    "    'Aligned_Tremor_Score': 'PKG Tremor Score'\n",
    "}\n",
    "OSCILLATORY_METRICS_COLS = {\n",
    "    'Beta_Peak_Power_at_DominantFreq': 'Beta Peak Power',\n",
    "    'Gamma_Peak_Power_at_DominantFreq': 'Gamma Peak Power'\n",
    "}\n",
    "APERIODIC_METRICS_TO_PLOT = ['Exponent_BestModel'] # Used in daily exponent plots\n",
    "\n",
    "# Key Column Names\n",
    "CHANNEL_COL = 'Channel'\n",
    "CHANNEL_DISPLAY_COL = 'Channel_Display'\n",
    "FOOOF_FREQ_BAND_COL = 'FreqRangeLabel'\n",
    "CLINICAL_STATE_COL = 'Clinical_State_2min_Window'\n",
    "CLINICAL_STATE_AGGREGATED_COL = 'Clinical_State_Aggregated'\n",
    "\n",
    "# --- Part 5: Analysis and Plotting Parameters ---\n",
    "# Ordering for Iterations and Plots\n",
    "ORDERED_FREQ_LABELS = [\"LowFreq\", \"MidFreq\", \"WideFreq\"]\n",
    "# Note: ORDERED_CHANNEL_LABELS is derived from data in the original script.\n",
    "# We will define the mapping here for consistency.\n",
    "CHANNEL_ORDER_MAP = {\n",
    "    'STN_DBS_2-0': 0, 'STN_DBS_3-1': 1, \n",
    "    'Cortical_ECoG_10-8': 2, 'Cortical_ECoG_11-9': 3\n",
    "}\n",
    "CHANNEL_ORDER_LIST = ['STN_DBS_2-0', 'STN_DBS_3-1', 'Cortical_ECoG_10-8', 'Cortical_ECoG_11-9']\n",
    "CHANNEL_GROUP_MAP = {'STN': ['STN_DBS_2-0', 'STN_DBS_3-1'], 'M1': ['Cortical_ECoG_10-8', 'Cortical_ECoG_11-9']}\n",
    "\n",
    "\n",
    "# Statistical Thresholds\n",
    "P_VALUE_THRESHOLD = 0.05\n",
    "MIN_SAMPLES_FOR_CORR = 5\n",
    "MIN_SAMPLES_FOR_GROUP_COMPARISON = 5\n",
    "R2_FILTER_THRESHOLD = 0.5\n",
    "\n",
    "# Clinical State Definitions and Ordering\n",
    "TARGET_CLINICAL_STATES_ORDERED = [\"Immobile\", \"Non-Dyskinetic Mobile\", \"Transitional Mobile\", \"Dyskinetic Mobile\"]\n",
    "ALL_CLINICAL_STATES_ORDERED = [\"Sleep\", \"Immobile\", \"Non-Dyskinetic Mobile\", \"Transitional Mobile\", \"Dyskinetic Mobile\"]\n",
    "SYMPTOM_ORDER = ['PKG BK Score', 'PKG DK Score', 'PKG Tremor Score']\n",
    "SYMPTOM_LEGEND_MAP = {'PKG BK Score': 'Bradykinesia', 'PKG DK Score': 'Dyskinesia', 'PKG Tremor Score': 'Tremor'}\n",
    "SYMPTOM_DISPLAY_ORDER = ['Bradykinesia', 'Dyskinesia', 'Tremor']\n",
    "\n",
    "# Daily Plot Parameters\n",
    "SF_TZ = pytz.timezone('America/Los_Angeles')\n",
    "PLOTTING_INTERVAL_MINUTES = 10\n",
    "MIN_POINTS_FOR_CI = 2\n",
    "CONFIDENCE_LEVEL_CI = 0.95\n",
    "GAP_THRESHOLD_BINS = 2\n",
    "\n",
    "# MLR Analysis Toggles\n",
    "ANALYZE_ALL_FREQ_BANDS_GLOBAL = False\n",
    "TARGET_FREQ_BAND_GLOBAL = \"WideFreq\"\n",
    "ANALYZE_ALL_FREQ_BANDS_STATE_SPECIFIC = False\n",
    "TARGET_FREQ_BAND_STATE_SPECIFIC = \"WideFreq\"\n",
    "\n",
    "\n",
    "# --- Part 6: Global Plotting Style Configuration ---\n",
    "# Seaborn and Matplotlib Global Theme\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 18\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 600\n",
    "\n",
    "# Color Palettes\n",
    "BASE_COLOR_PALETTE = {\n",
    "    'Exponent_BestModel': 'darkslateblue',\n",
    "    'Offset_BestModel': 'mediumseagreen',\n",
    "    'Beta_Peak_Power_at_DominantFreq': 'goldenrod',\n",
    "    'Gamma_Peak_Power_at_DominantFreq': 'firebrick',\n",
    "    'Aligned_BK': 'steelblue',\n",
    "    'Aligned_DK': 'orangered',\n",
    "    'Aligned_Tremor_Score': 'mediumpurple'\n",
    "}\n",
    "CLINICAL_STATE_COLORS = {\n",
    "    'Immobile': '#40E0D0',              # Turquoise\n",
    "    'Non-Dyskinetic Mobile': '#32CD32', # LimeGreen\n",
    "    'Transitional Mobile': '#FFD700',   # Gold\n",
    "    'Dyskinetic Mobile': '#FF6347',     # Tomato\n",
    "    'Sleep': '#4169E1',                 # RoyalBlue\n",
    "    'Other': '#C0C0C0',                 # Silver\n",
    "    'Mobile (All Types)': 'darkgreen'\n",
    "}\n",
    "PKG_SYMPTOM_COLORS = {\n",
    "    'Aligned_BK': BASE_COLOR_PALETTE.get('Aligned_BK', 'steelblue'),\n",
    "    'Aligned_DK': BASE_COLOR_PALETTE.get('Aligned_DK', 'orangered'),\n",
    "    'Aligned_Tremor_Score': BASE_COLOR_PALETTE.get('Aligned_Tremor_Score', 'mediumpurple')\n",
    "}\n",
    "\n",
    "# Other Plotting Style Constants\n",
    "DOT_ALPHA = 0.5\n",
    "REG_CI_ALPHA = 0.15\n",
    "BOX_FILL_ALPHA = 0.6\n",
    "BOXPLOT_LINE_THICKNESS = 2.25\n",
    "REG_LINE_THICKNESS = 2.0\n",
    "SIGNIFICANT_P_VAL_BG_COLOR = 'khaki'\n",
    "DEFAULT_P_VAL_BG_COLOR = 'ivory'\n",
    "\n",
    "print(\"Cell 1: All imports and global parameters have been defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c2fa4-d63e-4b52-bf13-195f0aaf1f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 2: User Input, File Paths, and Analysis Parameter Definitions ---\n",
    "\n",
    "# --- User Input ---\n",
    "# For development, hardcode; replace with input() in practice if needed for batch processing setup\n",
    "# This script processes ONE patient-hemisphere at a time.\n",
    "# The patient_hemisphere_id should match the identifier in the MASTER_FOOOF_PKG_results file name.\n",
    "# Example: \"RCS20R\" if your file is MASTER_FOOOF_PKG_results_RCS20R_... .csv\n",
    "patient_hemisphere_id = \"RCS20R\" # <<< USER: SET THIS FOR THE CURRENT PATIENT-HEMISPHERE\n",
    "print(f\"Processing data for Patient-Hemisphere ID: {patient_hemisphere_id}\")\n",
    "\n",
    "if not patient_hemisphere_id:\n",
    "    raise ValueError(\"Patient-Hemisphere ID cannot be empty.\")\n",
    "\n",
    "# --- Path Configuration ---\n",
    "print(f\"Project base path determined as: {project_base_path}\")\n",
    "print(f\"Attempting to load master data from: {master_csv_path_to_load}\")\n",
    "print(f\"Step 4 plots will be saved in: {analysis_session_plot_folder_step4}\")\n",
    "\n",
    "DOT_ALPHA_STEP4 = 0.5\n",
    "REG_CI_ALPHA_STEP4 = 0.15\n",
    "BOX_FILL_ALPHA_STEP4 = 0.6\n",
    "REG_LINE_THICKNESS_STEP4 = 2.0\n",
    "\n",
    "print(f\"\\nStep 4 analysis parameters and paths configured for {patient_hemisphere_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c32f9f-b3f7-41c9-991b-ad61c00369f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 3: Data Loading and Initial Preprocessing ---\n",
    "\n",
    "def load_and_preprocess_step4_data(file_path, patient_hemisphere_id_val):\n",
    "    \"\"\"Loads and preprocesses the master CSV file from Step 3 for Step 4 analyses.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"ERROR: Master CSV file from Step 3 not found at {file_path}\")\n",
    "        return None, []\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded {file_path}. Initial shape: {df.shape}\")\n",
    "\n",
    "        # --- Verify Patient ID consistency (optional but good check) ---\n",
    "        if 'SessionID' in df.columns and df['SessionID'].nunique() == 1:\n",
    "            csv_session_id = df['SessionID'].unique()[0]\n",
    "            if csv_session_id != patient_hemisphere_id_val:\n",
    "                print(f\"Warning: SessionID in CSV ({csv_session_id}) differs from expected ({patient_hemisphere_id_val}). Proceeding with CSV data.\")\n",
    "        elif 'SessionID' not in df.columns:\n",
    "             print(f\"Warning: 'SessionID' column not found. Adding it based on patient_hemisphere_id_val: {patient_hemisphere_id_val}\")\n",
    "             df['SessionID'] = patient_hemisphere_id_val\n",
    "\n",
    "\n",
    "        # --- Data Type Conversions & Cleaning ---\n",
    "        cols_to_numeric = (\n",
    "            list(APERIODIC_METRICS_COLS.keys()) +\n",
    "            list(PKG_METRICS_COLS.keys()) +\n",
    "            list(OSCILLATORY_METRICS_COLS.keys()) +\n",
    "            ['Total_Daily_LEDD_mg', 'R2_BestModel', 'Error_BestModel', 'Num_Peaks_BestModel']\n",
    "        )\n",
    "        for col in cols_to_numeric:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            else:\n",
    "                print(f\"Warning: Expected numeric column '{col}' not found in master_df.\")\n",
    "        \n",
    "        # Ensure key categorical columns are strings\n",
    "        for col in [CHANNEL_COL, CHANNEL_DISPLAY_COL, FOOOF_FREQ_BAND_COL, CLINICAL_STATE_COL, CLINICAL_STATE_AGGREGATED_COL, 'Hemisphere', 'BestModel_AperiodicMode']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str)\n",
    "            else:\n",
    "                print(f\"Warning: Expected categorical column '{col}' not found.\")\n",
    "\n",
    "\n",
    "        # --- Filtering based on FOOOF fit quality (optional, as done in original Step 4) ---\n",
    "        initial_rows = len(df)\n",
    "        if 'R2_BestModel' in df.columns:\n",
    "            r2_threshold_step4 = 0.5 # Example threshold\n",
    "            df = df[df['R2_BestModel'] >= r2_threshold_step4].copy()\n",
    "            print(f\"Filtered by R2_BestModel >= {r2_threshold_step4}. Rows changed from {initial_rows} to {len(df)}.\")\n",
    "        else:\n",
    "            print(\"Warning: 'R2_BestModel' column not found. Cannot filter by FOOOF fit quality.\")\n",
    "\n",
    "        # --- Create Channel_Display if not present (mapping from Step 3 should have done this) ---\n",
    "        # This is a fallback\n",
    "        if CHANNEL_DISPLAY_COL not in df.columns and CHANNEL_COL in df.columns:\n",
    "            print(f\"'{CHANNEL_DISPLAY_COL}' not found. Creating from '{CHANNEL_COL}'. Hardcoded map will be used if available.\")\n",
    "            # Hardcoded map (ensure this is consistent with Step 3 and your data)\n",
    "            channel_mapping_step4 = {\n",
    "                'TD_key0': 'STN_DBS_2-0', 'TD_key1': 'STN_DBS_3-1',\n",
    "                'TD_key2': 'Cortical_ECoG_10-8', 'TD_key3': 'Cortical_ECoG_11-9'\n",
    "            }\n",
    "            df[CHANNEL_DISPLAY_COL] = df[CHANNEL_COL].map(channel_mapping_step4).fillna(df[CHANNEL_COL])\n",
    "\n",
    "\n",
    "        # Determine ordered channel labels from the data if not hardcoded in Cell 2\n",
    "        if CHANNEL_DISPLAY_COL in df.columns:\n",
    "            def natural_sort_key_step4(s):\n",
    "                return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', str(s))]\n",
    "            \n",
    "            # Use pre-defined channel mapping for ordering if available\n",
    "            defined_channel_map_order = {\n",
    "                'STN_DBS_2-0': 0, 'STN_DBS_3-1': 1, \n",
    "                'Cortical_ECoG_10-8': 2, 'Cortical_ECoG_11-9': 3\n",
    "            }\n",
    "            unique_ch_labels_data = df[CHANNEL_DISPLAY_COL].unique()\n",
    "            # Filter and sort based on the defined order\n",
    "            ordered_ch_labels_from_data = sorted(\n",
    "                [ch for ch in unique_ch_labels_data if ch in defined_channel_map_order],\n",
    "                key=lambda x: defined_channel_map_order[x]\n",
    "            )\n",
    "            # Add any channels from data not in defined_channel_map_order, sorted naturally\n",
    "            ordered_ch_labels_from_data.extend(\n",
    "                sorted([ch for ch in unique_ch_labels_data if ch not in defined_channel_map_order], \n",
    "                       key=natural_sort_key_step4)\n",
    "            )\n",
    "            print(f\"Derived ORDERED_CHANNEL_LABELS for plots: {ordered_ch_labels_from_data}\")\n",
    "        else:\n",
    "            print(f\"ERROR: '{CHANNEL_DISPLAY_COL}' not found. Cannot determine channel order.\")\n",
    "            ordered_ch_labels_from_data = []\n",
    "            \n",
    "        # Drop rows where essential metrics for correlation/regression are NaN\n",
    "        # This step is crucial as analysis loops will iterate through these.\n",
    "        key_metrics_for_analysis = (\n",
    "            list(APERIODIC_METRICS_COLS.keys()) +\n",
    "            list(PKG_METRICS_COLS.keys()) +\n",
    "            list(OSCILLATORY_METRICS_COLS.keys())\n",
    "        )\n",
    "        key_metrics_present = [col for col in key_metrics_for_analysis if col in df.columns]\n",
    "        \n",
    "        rows_before_na_essential_drop = len(df)\n",
    "        if key_metrics_present:\n",
    "            df.dropna(subset=key_metrics_present, how='any', inplace=True) # Drop if ANY of these are NaN for a row\n",
    "            print(f\"Dropped rows with NaNs in any of {key_metrics_present}. Rows changed from {rows_before_na_essential_drop} to {len(df)}.\")\n",
    "        else:\n",
    "            print(\"Warning: No key metrics found to check for NaNs. Data might be incomplete.\")\n",
    "\n",
    "\n",
    "        print(f\"Final master_df for Step 4 shape: {df.shape}\")\n",
    "        if df.empty:\n",
    "            print(\"Warning: DataFrame is empty after preprocessing. Subsequent analyses might fail.\")\n",
    "        \n",
    "        return df, ordered_ch_labels_from_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing the CSV file '{file_path}' for Step 4: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        return None, []\n",
    "\n",
    "# --- Load and Preprocess Data ---\n",
    "master_df_step4, ORDERED_CHANNEL_LABELS = load_and_preprocess_step4_data(master_csv_path_to_load, patient_hemisphere_id)\n",
    "\n",
    "if master_df_step4 is not None and not master_df_step4.empty:\n",
    "    print(\"\\nFirst 5 rows of the processed master DataFrame for Step 4 (showing key cols):\")\n",
    "    cols_to_show_step4 = [CHANNEL_DISPLAY_COL, FOOOF_FREQ_BAND_COL] + \\\n",
    "                         list(APERIODIC_METRICS_COLS.keys()) + \\\n",
    "                         list(PKG_METRICS_COLS.keys()) + \\\n",
    "                         list(OSCILLATORY_METRICS_COLS.keys()) + \\\n",
    "                         ['Total_Daily_LEDD_mg']\n",
    "    cols_to_show_step4_present = [col for col in cols_to_show_step4 if col in master_df_step4.columns]\n",
    "    print(master_df_step4[cols_to_show_step4_present].head())\n",
    "else:\n",
    "    print(\"Halting Step 4 script as master data could not be loaded or is empty after preprocessing.\")\n",
    "    # sys.exit() # Uncomment to halt execution if master_df_step4 is not loaded\n",
    "\n",
    "print(\"\\nCell 3: Data loading and preprocessing complete for Step 4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b8a5c-369e-4ab0-a96e-5b893b88dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 4: Helper Functions for Step 4 ---\n",
    "\n",
    "def calculate_spearman_with_n(data_df, col1, col2, min_samples=MIN_SAMPLES_FOR_CORR):\n",
    "    \"\"\"Calculates Spearman correlation if N >= min_samples.\"\"\"\n",
    "    pair_data = data_df[[col1, col2]].dropna()\n",
    "    n_points = len(pair_data)\n",
    "\n",
    "    if n_points < min_samples:\n",
    "        return np.nan, np.nan, n_points # rho, p-value, N\n",
    "    try:\n",
    "        rho, p_value = spearmanr(pair_data[col1], pair_data[col2])\n",
    "        if np.isnan(rho): # Handle cases where spearmanr might return NaN (e.g., no variance)\n",
    "            return np.nan, np.nan, n_points\n",
    "        return rho, p_value, n_points\n",
    "    except ValueError: # Handle other potential errors like constant input\n",
    "        return np.nan, np.nan, n_points\n",
    "\n",
    "def calculate_partial_spearman(data_df, x_col, y_col, covar_cols, min_samples=MIN_SAMPLES_FOR_CORR):\n",
    "    \"\"\"Calculates partial Spearman correlation if N >= min_samples.\"\"\"\n",
    "    all_cols_for_partial = [x_col, y_col] + covar_cols\n",
    "    partial_data = data_df[all_cols_for_partial].dropna()\n",
    "    n_points = len(partial_data)\n",
    "\n",
    "    if n_points < min_samples:\n",
    "        return np.nan, np.nan, n_points # partial_rho, p-value, N\n",
    "    try:\n",
    "        # Ensure all columns for partial corr are numeric and not constant after dropna\n",
    "        if not all(partial_data[col].nunique() > 1 for col in all_cols_for_partial if col in partial_data):\n",
    "            # print(f\"Warning: Constant column found in data for partial corr {x_col} vs {y_col}. N={n_points}\")\n",
    "            return np.nan, np.nan, n_points\n",
    "            \n",
    "        pcorr_result = pg.partial_corr(data=partial_data, x=x_col, y=y_col, covar=covar_cols, method='spearman')\n",
    "        rho = pcorr_result['r'].iloc[0]\n",
    "        p_value = pcorr_result['p-val'].iloc[0]\n",
    "        return rho, p_value, n_points\n",
    "    except Exception as e: # Catch any error during partial correlation\n",
    "        # print(f\"Error in partial correlation for {x_col} vs {y_col} (N={n_points}): {e}\")\n",
    "        return np.nan, np.nan, n_points\n",
    "\n",
    "\n",
    "def annotate_correlation_on_plot(ax, rho, p_value, N_val, test_type=\"Spearman ρ\", \n",
    "                                 x_pos=0.97, y_pos=0.97, fontsize=9,\n",
    "                                 sig_threshold=P_VALUE_THRESHOLD):\n",
    "    \"\"\"Annotates correlation statistics on a plot axis.\"\"\"\n",
    "    if pd.isna(rho) or pd.isna(p_value):\n",
    "        stat_text = f\"{test_type}: N/A (N={N_val})\"\n",
    "        bg_color = DEFAULT_P_VAL_BG_COLOR_STEP4 # from Cell 2\n",
    "    else:\n",
    "        stars = \"\"\n",
    "        if p_value < 0.001: stars = \"***\"\n",
    "        elif p_value < 0.01: stars = \"**\"\n",
    "        elif p_value < sig_threshold: stars = \"*\"\n",
    "        stat_text = f\"{test_type}={rho:.2f}{stars}\\np={p_value:.3g}\\n(N={N_val})\"\n",
    "        bg_color = SIGNIFICANT_P_VAL_BG_COLOR_STEP4 if p_value < sig_threshold else DEFAULT_P_VAL_BG_COLOR_STEP4\n",
    "    \n",
    "    ax.text(x_pos, y_pos, stat_text, transform=ax.transAxes, fontsize=fontsize,\n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', fc=bg_color, alpha=0.85, edgecolor='darkgrey'))\n",
    "\n",
    "def get_safe_filename_step4(base_name):\n",
    "    \"\"\"Creates a filesystem-safe filename.\"\"\"\n",
    "    return re.sub(r'[^\\w\\s-]', '', str(base_name)).strip().replace(' ', '_').replace('-', '_')\n",
    "\n",
    "def trim_data_for_boxplot_visualization(df_group, value_col):\n",
    "    \"\"\"Trims outliers based on IQR for cleaner boxplot visualization (doesn't affect stats).\"\"\"\n",
    "    if df_group.empty or df_group[value_col].isnull().all() or len(df_group) < 2:\n",
    "        return df_group\n",
    "    Q1 = df_group[value_col].quantile(0.25)\n",
    "    Q3 = df_group[value_col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    if IQR == 0: # Avoid issues if all data points are the same\n",
    "        return df_group\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df_group[(df_group[value_col] >= lower_bound) & (df_group[value_col] <= upper_bound)]\n",
    "\n",
    "\n",
    "print(\"Cell 4: Helper functions for Step 4 defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e498b-0341-4a81-bd36-f40089d1efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 5_PREAMBLE: Definitions for State-Specific Analyses (Revised Order: No Sleep, 4 Separate Mobile/Immobile States) ---\n",
    "\n",
    "# --- Define Target Clinical States and their Order (4 States, No Sleep, New Order) ---\n",
    "TARGET_CLINICAL_STATES_ORDERED = [\n",
    "    \"Immobile\",\n",
    "    \"Non-Dyskinetic Mobile\",\n",
    "    \"Transitional Mobile\",\n",
    "    \"Dyskinetic Mobile\"\n",
    "]\n",
    "\n",
    "ORIGINAL_STATES_FOR_ANALYSIS = TARGET_CLINICAL_STATES_ORDERED[:]\n",
    "\n",
    "# No combining needed for this setup\n",
    "STATES_TO_COMBINE_MAPPING = {}\n",
    "NEW_COMBINED_STATE_NAME = None\n",
    "\n",
    "\n",
    "# --- Define Clinical State Colors (using original distinct colors, excluding Sleep) ---\n",
    "# The color definitions themselves don't change, but their application order will follow TARGET_CLINICAL_STATES_ORDERED\n",
    "NEW_CLINICAL_STATE_COLORS_FOR_PLOTTING = {\n",
    "    'Immobile': '#40E0D0',              # Turquoise\n",
    "    'Non-Dyskinetic Mobile': '#32CD32', # LimeGreen\n",
    "    'Transitional Mobile': '#FFD700',   # Gold\n",
    "    'Dyskinetic Mobile': '#FF6347',     # Tomato\n",
    "    # Fallbacks or other states if they were to appear unexpectedly\n",
    "    'Sleep': '#4169E1',                 # RoyalBlue (original, but excluded from TARGET_CLINICAL_STATES_ORDERED)\n",
    "    'Other': '#C0C0C0',                 # Silver\n",
    "    'Mobile (All Types)': 'darkgreen'   # For aggregated view if ever used\n",
    "}\n",
    "\n",
    "\n",
    "# --- Define PKG Symptom Colors (remains the same) ---\n",
    "PKG_SYMPTOM_COLORS = {\n",
    "    'Aligned_BK': BASE_COLOR_PALETTE.get('Aligned_BK', 'steelblue'),\n",
    "    'Aligned_DK': BASE_COLOR_PALETTE.get('Aligned_DK', 'orangered'),\n",
    "    'Aligned_Tremor_Score': BASE_COLOR_PALETTE.get('Aligned_Tremor_Score', 'mediumpurple')\n",
    "}\n",
    "\n",
    "# Base output directory name remains based on the content (4 states, no sleep)\n",
    "STATE_SPECIFIC_ANALYSIS_DIR = os.path.join(analysis_session_plot_folder_step4)\n",
    "os.makedirs(STATE_SPECIFIC_ANALYSIS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Cell 5_PREAMBLE: Definitions for state-specific analyses (4 States - No Sleep, Separate Mobile, New Order) are set.\")\n",
    "print(f\"Target clinical states for analysis (NEW ORDER): {TARGET_CLINICAL_STATES_ORDERED}\")\n",
    "print(f\"Colors for clinical states: {NEW_CLINICAL_STATE_COLORS_FOR_PLOTTING}\") # This dict remains the same, order of use changes\n",
    "print(f\"State-specific outputs will be saved in subdirectories of: {STATE_SPECIFIC_ANALYSIS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df28e92-33a3-4a24-9544-6334616c6e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 5A (Revised): State-Specific Correlations with FDR Correction ---\n",
    "# This version calculates all bivariate and partial correlations for each of the\n",
    "# TARGET_CLINICAL_STATES_ORDERED, pools all p-values, and applies a single\n",
    "# Benjamini-Hochberg FDR correction to the entire family of tests.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "print(\"\\\\n--- Cell 5A (Revised): Starting State-Specific Correlation Calculations with FDR Correction ---\")\n",
    "\n",
    "# This check ensures that the main dataframe from Cell 3 is loaded and available.\n",
    "if 'master_df_step4' not in locals() or master_df_step4.empty:\n",
    "    print(\"CRITICAL ERROR: master_df_step4 not available or empty. Cannot proceed with Cell 5A. Please run prior cells.\")\n",
    "    # In a real run, you might use sys.exit() here. For now, we'll let it error out if df is missing.\n",
    "\n",
    "# --- Step 1: Data Preparation and Filtering ---\n",
    "# Filter the master dataframe to include only the clinical states relevant to this analysis.\n",
    "# TARGET_CLINICAL_STATES_ORDERED is defined in Cell 5_PREAMBLE.\n",
    "master_df_step4_filtered_states = master_df_step4[master_df_step4[CLINICAL_STATE_COL].isin(TARGET_CLINICAL_STATES_ORDERED)].copy()\n",
    "\n",
    "if master_df_step4_filtered_states.empty:\n",
    "    print(f\"No data found after filtering for target clinical states: {TARGET_CLINICAL_STATES_ORDERED}. Skipping Cell 5A.\")\n",
    "else:\n",
    "    # Ensure the clinical state column is treated as an ordered categorical variable for consistency.\n",
    "    master_df_step4_filtered_states[CLINICAL_STATE_COL] = pd.Categorical(\n",
    "        master_df_step4_filtered_states[CLINICAL_STATE_COL],\n",
    "        categories=TARGET_CLINICAL_STATES_ORDERED,\n",
    "        ordered=True\n",
    "    )\n",
    "    master_df_step4_filtered_states.dropna(subset=[CLINICAL_STATE_COL], inplace=True)\n",
    "    print(f\"Filtered data for target clinical states. Shape: {master_df_step4_filtered_states.shape}.\")\n",
    "\n",
    "    # Define the output directory for the correlation result CSVs.\n",
    "    state_corr_csv_dir = os.path.join(STATE_SPECIFIC_ANALYSIS_DIR, \"Correlation_CSVs_by_State_FDR_Corrected\")\n",
    "    os.makedirs(state_corr_csv_dir, exist_ok=True)\n",
    "\n",
    "    # --- Step 2: Collect P-Values from All Planned Tests ---\n",
    "    # This list will store the raw results of EVERY single correlation test before FDR correction.\n",
    "    all_correlation_results = []\n",
    "    \n",
    "    # These loops iterate through every combination of state, channel, and frequency band.\n",
    "    print(\"\\\\nCalculating all correlations across states, channels, and frequency bands...\")\n",
    "    for state_current in TARGET_CLINICAL_STATES_ORDERED:\n",
    "        df_state = master_df_step4_filtered_states[master_df_step4_filtered_states[CLINICAL_STATE_COL] == state_current]\n",
    "        if df_state.empty:\n",
    "            continue\n",
    "\n",
    "        for channel_label in ORDERED_CHANNEL_LABELS:\n",
    "            df_channel_state = df_state[df_state[CHANNEL_DISPLAY_COL] == channel_label]\n",
    "            if df_channel_state.empty:\n",
    "                continue\n",
    "\n",
    "            for freq_label in ORDERED_FREQ_LABELS:\n",
    "                df_channel_freq_state = df_channel_state[df_channel_state[FOOOF_FREQ_BAND_COL] == freq_label].copy()\n",
    "                if df_channel_freq_state.empty:\n",
    "                    continue\n",
    "\n",
    "                # --- Test Family 1: Bivariate Aperiodic vs. PKG ---\n",
    "                for ap_col, ap_name in APERIODIC_METRICS_COLS.items():\n",
    "                    for pkg_col, pkg_name in PKG_METRICS_COLS.items():\n",
    "                        if ap_col in df_channel_freq_state.columns and pkg_col in df_channel_freq_state.columns:\n",
    "                            # CHANGE: Using all valid data, no exclusion of zeros\n",
    "                            rho, p_val, N = calculate_spearman_with_n(df_channel_freq_state, ap_col, pkg_col)\n",
    "                            all_correlation_results.append({\n",
    "                                'TestType': 'Bivar_AP_PKG', 'ClinicalState': state_current, 'Channel': channel_label, 'FreqBand': freq_label,\n",
    "                                'Metric1': ap_name, 'Metric2': pkg_name, 'Rho': rho, 'P_Value_Original': p_val, 'N': N\n",
    "                            })\n",
    "\n",
    "                # --- Test Family 2: Partial Aperiodic vs. PKG (controlling for Oscillatory) ---\n",
    "                covariates = [col for col in OSCILLATORY_METRICS_COLS.keys() if col in df_channel_freq_state.columns]\n",
    "                if len(covariates) == 2:\n",
    "                    for ap_col, ap_name in APERIODIC_METRICS_COLS.items():\n",
    "                        for pkg_col, pkg_name in PKG_METRICS_COLS.items():\n",
    "                            if ap_col in df_channel_freq_state.columns and pkg_col in df_channel_freq_state.columns:\n",
    "                                # CHANGE: Using all valid data\n",
    "                                partial_rho, partial_p_val, N_partial = calculate_partial_spearman(\n",
    "                                    df_channel_freq_state, ap_col, pkg_col, covariates\n",
    "                                )\n",
    "                                all_correlation_results.append({\n",
    "                                    'TestType': 'Partial_AP_PKG', 'ClinicalState': state_current, 'Channel': channel_label, 'FreqBand': freq_label,\n",
    "                                    'Metric1': ap_name, 'Metric2': pkg_name, 'Rho': partial_rho, 'P_Value_Original': partial_p_val, 'N': N_partial\n",
    "                                })\n",
    "\n",
    "                # --- Test Family 3: Bivariate Aperiodic vs. Oscillatory ---\n",
    "                for ap_col, ap_name in APERIODIC_METRICS_COLS.items():\n",
    "                    for osc_col, osc_name in OSCILLATORY_METRICS_COLS.items():\n",
    "                        if ap_col in df_channel_freq_state.columns and osc_col in df_channel_freq_state.columns:\n",
    "                            rho_ap_osc, p_val_ap_osc, N_ap_osc = calculate_spearman_with_n(df_channel_freq_state, ap_col, osc_col)\n",
    "                            all_correlation_results.append({\n",
    "                                'TestType': 'Bivar_AP_Osc', 'ClinicalState': state_current, 'Channel': channel_label, 'FreqBand': freq_label,\n",
    "                                'Metric1': ap_name, 'Metric2': osc_name, 'Rho': rho_ap_osc, 'P_Value_Original': p_val_ap_osc, 'N': N_ap_osc\n",
    "                            })\n",
    "\n",
    "    # --- Step 3: Apply Benjamini-Hochberg FDR Correction ---\n",
    "    print(f\"\\\\nCollected a total of {len(all_correlation_results)} p-values for FDR correction.\")\n",
    "    \n",
    "    if not all_correlation_results:\n",
    "        print(\"No correlation results were generated. Cannot perform FDR correction.\")\n",
    "    else:\n",
    "        df_all_results = pd.DataFrame(all_correlation_results)\n",
    "        \n",
    "        # Isolate non-NaN p-values for correction\n",
    "        p_values_to_correct = df_all_results['P_Value_Original'].dropna()\n",
    "        \n",
    "        if p_values_to_correct.empty:\n",
    "            print(\"No valid p-values to correct. All correlations may have had insufficient data.\")\n",
    "            df_all_results['P_Value_FDR_Adjusted'] = np.nan\n",
    "            df_all_results['Significant_FDR_0.05'] = False\n",
    "        else:\n",
    "            # The fdrcorrection function from statsmodels handles the BH procedure\n",
    "            rejected, pvals_corrected = fdrcorrection(p_values_to_correct, alpha=0.05, method='indep', is_sorted=False)\n",
    "            \n",
    "            # Create a temporary series with the corrected p-values, matching the index of the original non-NaN p-values\n",
    "            corrected_p_series = pd.Series(pvals_corrected, index=p_values_to_correct.index)\n",
    "            \n",
    "            # Map the corrected values back to the original dataframe. This correctly handles NaNs.\n",
    "            df_all_results['P_Value_FDR_Adjusted'] = corrected_p_series\n",
    "            df_all_results['Significant_FDR_0.05'] = df_all_results['P_Value_FDR_Adjusted'] < 0.05\n",
    "\n",
    "        print(\"FDR correction applied successfully.\")\n",
    "\n",
    "        # --- Step 4: Separate and Save Final Results ---\n",
    "        # Separate the combined results dataframe back into the three original test types\n",
    "        \n",
    "        # 1. Bivariate AP vs PKG\n",
    "        df_bivar_ap_pkg_final = df_all_results[df_all_results['TestType'] == 'Bivar_AP_PKG'].copy()\n",
    "        df_bivar_ap_pkg_final.rename(columns={'Metric1': 'AperiodicMetric', 'Metric2': 'PKGMetric', 'Rho': 'SpearmanRho'}, inplace=True)\n",
    "        df_bivar_ap_pkg_final = df_bivar_ap_pkg_final.drop(columns='TestType')\n",
    "        df_bivar_ap_pkg_final.to_csv(os.path.join(state_corr_csv_dir, f\"{patient_hemisphere_id}_Bivariate_AP_vs_PKG_ByState_FDR_AllData.csv\"), index=False)\n",
    "        print(f\"\\\\nSaved FDR-Corrected Bivariate AP vs PKG results (all data) for {patient_hemisphere_id}.\")\n",
    "\n",
    "        # 2. Partial AP vs PKG\n",
    "        df_partial_ap_pkg_final = df_all_results[df_all_results['TestType'] == 'Partial_AP_PKG'].copy()\n",
    "        df_partial_ap_pkg_final.rename(columns={'Metric1': 'AperiodicMetric', 'Metric2': 'PKGMetric', 'Rho': 'PartialSpearmanRho_vs_BetaGamma'}, inplace=True)\n",
    "        df_partial_ap_pkg_final = df_partial_ap_pkg_final.drop(columns='TestType')\n",
    "        df_partial_ap_pkg_final.to_csv(os.path.join(state_corr_csv_dir, f\"{patient_hemisphere_id}_Partial_AP_vs_PKG_ByState_FDR_AllData.csv\"), index=False)\n",
    "        print(f\"Saved FDR-Corrected Partial AP vs PKG results (all data) for {patient_hemisphere_id}.\")\n",
    "\n",
    "        # 3. Bivariate AP vs Oscillatory\n",
    "        df_bivar_ap_osc_final = df_all_results[df_all_results['TestType'] == 'Bivar_AP_Osc'].copy()\n",
    "        df_bivar_ap_osc_final.rename(columns={'Metric1': 'AperiodicMetric', 'Metric2': 'OscillatoryMetric', 'Rho': 'SpearmanRho'}, inplace=True)\n",
    "        df_bivar_ap_osc_final = df_bivar_ap_osc_final.drop(columns='TestType')\n",
    "        df_bivar_ap_osc_final.to_csv(os.path.join(state_corr_csv_dir, f\"{patient_hemisphere_id}_Bivariate_AP_vs_Oscillatory_ByState_FDR_AllData.csv\"), index=False)\n",
    "        print(f\"Saved FDR-Corrected Bivariate AP vs Oscillatory results (all data) for {patient_hemisphere_id}.\")\n",
    "        \n",
    "        print(\"\\\\nSample of final FDR-corrected Bivariate AP vs PKG results:\")\n",
    "        print(df_bivar_ap_pkg_final.head())\n",
    "\n",
    "\n",
    "print(\"\\\\n--- Cell 5A (Revised) with FDR Correction Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c3778-f82a-46f0-a3f5-8bb2c959bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 5B (New): State-Specific Overview Scatter Plots (Aperiodic vs. PKG) (Revised: No Sleep, 4 Separate Mobile/Immobile States) ---\n",
    "# Generates overview scatter plots: one figure per (PKG_Symptom, Aperiodic_Metric, Channel, FreqBand),\n",
    "# with subplots for each clinical state (now 4 states). Y-axes are standardized within each figure.\n",
    "# Scatter plot points are 10-minute averages. Regression line uses all granular data.\n",
    "\n",
    "print(\"\\\\n--- Cell 5B (New): Starting State-Specific Overview Scatter Plot Generation (4 States - No Sleep, 10-min avg pts) ---\")\n",
    "\n",
    "# Ensure master_df_step4_filtered_states has the correct 4 states from the updated Cell 5A\n",
    "if 'master_df_step4_filtered_states' not in locals() or master_df_step4_filtered_states.empty:\n",
    "    print(\"master_df_step4_filtered_states (with 4 states - No Sleep) not available or empty. Skipping Cell 5B.\")\n",
    "elif 'df_bivar_ap_pkg_state' not in locals() or ('df_bivar_ap_pkg_state' in locals() and df_bivar_ap_pkg_state.empty):\n",
    "    print(\"Bivariate AP vs PKG by State correlation results (df_bivar_ap_pkg_state) not found. Skipping Cell 5B plot annotations.\")\n",
    "    df_bivar_ap_pkg_state = pd.DataFrame(columns=['ClinicalState', 'Channel', 'FreqBand', 'AperiodicMetric', 'PKGMetric', 'SpearmanRho', 'PValue', 'N'])\n",
    "else:\n",
    "    # UPDATED FOLDER NAME\n",
    "    plot_subdir_overview_scatter_state = os.path.join(STATE_SPECIFIC_ANALYSIS_DIR, \"Overview_Scatter_AP_vs_PKG_by_State\")\n",
    "    os.makedirs(plot_subdir_overview_scatter_state, exist_ok=True)\n",
    "\n",
    "    # Ensure 'datetime_for_avg' exists from Cell 5A modifications\n",
    "    if 'datetime_for_avg' not in master_df_step4_filtered_states.columns:\n",
    "        print(\"ERROR in Cell 5B: 'datetime_for_avg' column missing from master_df_step4_filtered_states. Averaging will fail.\")\n",
    "        # Fallback or exit\n",
    "    else:\n",
    "        for pkg_col_overview, pkg_name_overview in PKG_METRICS_COLS.items():\n",
    "            if pkg_col_overview not in master_df_step4_filtered_states.columns:\n",
    "                print(f\"PKG Metric {pkg_name_overview} ({pkg_col_overview}) not found in data. Skipping its overview plots.\")\n",
    "                continue\n",
    "\n",
    "            for ap_metric_overview_col, ap_metric_overview_name in APERIODIC_METRICS_COLS.items():\n",
    "                if ap_metric_overview_col not in master_df_step4_filtered_states.columns:\n",
    "                    print(f\"Aperiodic Metric {ap_metric_overview_name} ({ap_metric_overview_col}) not found. Skipping its overview plots.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\\\nGenerating overview plots for: {ap_metric_overview_name} vs. {pkg_name_overview}\")\n",
    "\n",
    "                for channel_label_overview in ORDERED_CHANNEL_LABELS:\n",
    "                    for freq_label_overview in ORDERED_FREQ_LABELS:\n",
    "                        \n",
    "                        fig_width = max(15, 4 * len(TARGET_CLINICAL_STATES_ORDERED)) # Now 4 states\n",
    "                        fig, axes = plt.subplots(1, len(TARGET_CLINICAL_STATES_ORDERED), \n",
    "                                                 figsize=(fig_width, 5.5), sharey=False)\n",
    "                        if len(TARGET_CLINICAL_STATES_ORDERED) == 1: # Should not happen with 4 states, but good practice\n",
    "                            axes = [axes]\n",
    "                        \n",
    "                        fig.suptitle(f\"{ap_metric_overview_name} vs. {pkg_name_overview}\\\\nChannel: {channel_label_overview} - Freq: {freq_label_overview} - Patient: {patient_hemisphere_id}\\\\n(Scatter points are 10-min averages; Regression on all raw data)\",\n",
    "                                     fontsize=plt.rcParams['figure.titlesize'] * 0.85, y=1.05)\n",
    "\n",
    "                        all_ap_values_for_ylim = []\n",
    "                        valid_plot_exists_for_figure = False \n",
    "\n",
    "                        for i, state_overview_ylim in enumerate(TARGET_CLINICAL_STATES_ORDERED): # Iterates 4 states\n",
    "                            df_current_combo_ylim_pass = master_df_step4_filtered_states[\n",
    "                                (master_df_step4_filtered_states[CLINICAL_STATE_COL] == state_overview_ylim) &\n",
    "                                (master_df_step4_filtered_states[CHANNEL_DISPLAY_COL] == channel_label_overview) &\n",
    "                                (master_df_step4_filtered_states[FOOOF_FREQ_BAND_COL] == freq_label_overview)\n",
    "                            ]\n",
    "                            cols_to_drop_na_for_ylim_pass = [ap_metric_overview_col, pkg_col_overview, 'datetime_for_avg']\n",
    "                            \n",
    "                            df_plot_data_ylim_pass = df_current_combo_ylim_pass.dropna(subset=cols_to_drop_na_for_ylim_pass)\n",
    "                            \n",
    "                            if not df_plot_data_ylim_pass.empty and len(df_plot_data_ylim_pass) >= MIN_SAMPLES_FOR_CORR:\n",
    "                                all_ap_values_for_ylim.extend(df_plot_data_ylim_pass[ap_metric_overview_col].tolist())\n",
    "                        \n",
    "                        min_y, max_y = (np.nan, np.nan)\n",
    "                        if all_ap_values_for_ylim: \n",
    "                            min_y_val_calc = np.nanmin(all_ap_values_for_ylim)\n",
    "                            max_y_val_calc = np.nanmax(all_ap_values_for_ylim)\n",
    "                            if not (np.isnan(min_y_val_calc) or np.isnan(max_y_val_calc)):\n",
    "                                 padding = (max_y_val_calc - min_y_val_calc) * 0.1 if (max_y_val_calc - min_y_val_calc) > 0 else 0.1\n",
    "                                 min_y = min_y_val_calc - padding\n",
    "                                 max_y = max_y_val_calc + padding\n",
    "                                 \n",
    "                        for i, state_overview in enumerate(TARGET_CLINICAL_STATES_ORDERED): # Iterates 4 states\n",
    "                            ax = axes[i]\n",
    "                            df_current_combo_plot = master_df_step4_filtered_states[\n",
    "                                (master_df_step4_filtered_states[CLINICAL_STATE_COL] == state_overview) &\n",
    "                                (master_df_step4_filtered_states[CHANNEL_DISPLAY_COL] == channel_label_overview) &\n",
    "                                (master_df_step4_filtered_states[FOOOF_FREQ_BAND_COL] == freq_label_overview)\n",
    "                            ]\n",
    "                            \n",
    "                            cols_to_drop_na_plot = [ap_metric_overview_col, pkg_col_overview, 'datetime_for_avg']\n",
    "                            df_plot_data_granular_plot = df_current_combo_plot.dropna(subset=cols_to_drop_na_plot)\n",
    "\n",
    "                            if not df_plot_data_granular_plot.empty and len(df_plot_data_granular_plot) >= MIN_SAMPLES_FOR_CORR :\n",
    "                                valid_plot_exists_for_figure = True \n",
    "\n",
    "                                corr_stats_row = df_bivar_ap_pkg_state[\n",
    "                                    (df_bivar_ap_pkg_state['ClinicalState'] == state_overview) &\n",
    "                                    (df_bivar_ap_pkg_state['Channel'] == channel_label_overview) &\n",
    "                                    (df_bivar_ap_pkg_state['FreqBand'] == freq_label_overview) &\n",
    "                                    (df_bivar_ap_pkg_state['AperiodicMetric'] == ap_metric_overview_name) &\n",
    "                                    (df_bivar_ap_pkg_state['PKGMetric'] == pkg_name_overview)\n",
    "                                ]\n",
    "                                rho = corr_stats_row['SpearmanRho'].iloc[0] if not corr_stats_row.empty else np.nan\n",
    "                                p_val = corr_stats_row['PValue'].iloc[0] if not corr_stats_row.empty else np.nan\n",
    "                                N_val = corr_stats_row['N'].iloc[0] if not corr_stats_row.empty else len(df_plot_data_granular_plot)\n",
    "\n",
    "                                df_averaged_points_plot = pd.DataFrame()\n",
    "                                if 'datetime_for_avg' in df_plot_data_granular_plot.columns and \\\n",
    "                                   not df_plot_data_granular_plot['datetime_for_avg'].isnull().all():\n",
    "                                    try:\n",
    "                                        df_averaged_points_plot = df_plot_data_granular_plot.set_index('datetime_for_avg')\\\n",
    "                                            .groupby(pd.Grouper(freq='10T'))[[ap_metric_overview_col, pkg_col_overview]]\\\n",
    "                                            .mean().dropna()\n",
    "                                    except Exception as e_avg: \n",
    "                                        print(f\"Warning: 10-min averaging failed for {channel_label_overview}, {freq_label_overview}, {state_overview}. Plotting granular points. Error: {e_avg}\")\n",
    "                                        df_averaged_points_plot = df_plot_data_granular_plot \n",
    "                                else: \n",
    "                                    df_averaged_points_plot = df_plot_data_granular_plot\n",
    "\n",
    "                                if not df_averaged_points_plot.empty:\n",
    "                                    sns.scatterplot(data=df_averaged_points_plot, x=pkg_col_overview, y=ap_metric_overview_col,\n",
    "                                                    color=NEW_CLINICAL_STATE_COLORS_FOR_PLOTTING.get(state_overview, 'grey'),\n",
    "                                                    alpha=DOT_ALPHA_STEP4 + 0.2, s=40, edgecolor='black', linewidths=0.5, ax=ax, legend=False)\n",
    "\n",
    "                                sns.regplot(data=df_plot_data_granular_plot, x=pkg_col_overview, y=ap_metric_overview_col, scatter=False, ax=ax,\n",
    "                                            line_kws={'color': 'black', 'linewidth': 1.5, 'alpha': 0.6})\n",
    "                                \n",
    "                                annotate_correlation_on_plot(ax, rho, p_val, N_val, fontsize=8)\n",
    "                                ax.set_title(state_overview, fontsize=plt.rcParams['axes.titlesize']*0.8)\n",
    "                                ax.set_xlabel(pkg_name_overview if i == len(TARGET_CLINICAL_STATES_ORDERED) // 2 else \"\", fontsize=plt.rcParams['axes.labelsize']*0.9)\n",
    "                                \n",
    "                                if not pd.isna(min_y) and not pd.isna(max_y):\n",
    "                                    ax.set_ylim(min_y, max_y)\n",
    "                            else:\n",
    "                                ax.text(0.5, 0.5, \"N < min_samples\" if len(df_plot_data_granular_plot) < MIN_SAMPLES_FOR_CORR else \"No Data\", \n",
    "                                        horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=9)\n",
    "                                ax.set_title(state_overview, fontsize=plt.rcParams['axes.titlesize']*0.8)\n",
    "                                if not pd.isna(min_y) and not pd.isna(max_y): \n",
    "                                    ax.set_ylim(min_y, max_y)\n",
    "\n",
    "                            if i == 0:\n",
    "                                ax.set_ylabel(ap_metric_overview_name, fontsize=plt.rcParams['axes.labelsize'])\n",
    "                            else:\n",
    "                                ax.set_ylabel(\"\")\n",
    "                                ax.set_yticklabels([])\n",
    "                            \n",
    "                            ax.tick_params(axis='x', labelsize=plt.rcParams['xtick.labelsize']*0.9)\n",
    "                            ax.tick_params(axis='y', labelsize=plt.rcParams['ytick.labelsize']*0.9)\n",
    "\n",
    "                        if valid_plot_exists_for_figure: \n",
    "                            plt.tight_layout(rect=[0, 0.03, 1, 0.93]) \n",
    "                            safe_ap_name = get_safe_filename_step4(ap_metric_overview_name)\n",
    "                            safe_pkg_name = get_safe_filename_step4(pkg_name_overview)\n",
    "                            safe_ch_name = get_safe_filename_step4(channel_label_overview)\n",
    "                            safe_freq_name = get_safe_filename_step4(freq_label_overview)\n",
    "                            # UPDATED FILENAME\n",
    "                            plot_filename = f\"Overview_{safe_ap_name}_vs_{safe_pkg_name}_{safe_ch_name}_{safe_freq_name}.png\" \n",
    "                            plt.savefig(os.path.join(plot_subdir_overview_scatter_state, plot_filename))\n",
    "                        plt.close(fig)\n",
    "\n",
    "print(\"\\\\n--- Cell 5B (New): State-Specific Overview Scatter Plot Generation (4 States - No Sleep, 10-min avg pts) Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d527b13-d2fd-4c9a-9d2a-2b5f26ba9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 5 (Revised V6 - With FDR Correction and All Data): Bivariate & Partial Correlations ---\n",
    "# Iterates through Channel_Display AND FreqRangeLabel\n",
    "\n",
    "print(\"\\\\n--- Cell 5 (Revised V6 - With FDR Correction and All Data): Starting Correlation Analyses ---\")\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "# --- Define new plotting parameters for this cell ---\n",
    "# Text smaller by 60% from the enlarged version (1.5 * 0.4 = 0.6)\n",
    "font_scale_factor = 0.6\n",
    "# Color for tremor remains green\n",
    "MODIFIED_COLOR_PALETTE = BASE_COLOR_PALETTE.copy()\n",
    "MODIFIED_COLOR_PALETTE['Aligned_Tremor_Score'] = 'green'\n",
    "\n",
    "SIGNIFICANT_P_VAL_BG_COLOR_STEP4 = 'khaki'\n",
    "DEFAULT_P_VAL_BG_COLOR_STEP4 = 'ivory'\n",
    "\n",
    "if 'master_df_step4' not in locals() or master_df_step4 is None or master_df_step4.empty:\n",
    "    print(\"master_df_step4 not available or empty. Skipping Cell 5.\")\n",
    "else:\n",
    "    # Create a working copy to add a datetime column for averaging\n",
    "    master_df_step4_processed_c5 = master_df_step4.copy()\n",
    "    if 'Aligned_PKG_UnixTimestamp' in master_df_step4_processed_c5.columns:\n",
    "        master_df_step4_processed_c5['datetime_for_avg_c5'] = pd.to_datetime(\n",
    "            master_df_step4_processed_c5['Aligned_PKG_UnixTimestamp'], unit='s', utc=True, errors='coerce'\n",
    "        )\n",
    "    else:\n",
    "        print(\"Warning in Cell 5: 'Aligned_PKG_UnixTimestamp' not found. Will plot granular points.\")\n",
    "        master_df_step4_processed_c5['datetime_for_avg_c5'] = pd.NaT\n",
    "\n",
    "    # Create subdirectories for plots from this cell\n",
    "    plot_subdir_bivariate_ap_pkg = os.path.join(analysis_session_plot_folder_step4, \"Bivariate_AP_vs_PKG\")\n",
    "    plot_subdir_bivariate_ap_osc = os.path.join(analysis_session_plot_folder_step4, \"Bivariate_AP_vs_Oscillatory\")\n",
    "    os.makedirs(plot_subdir_bivariate_ap_pkg, exist_ok=True)\n",
    "    os.makedirs(plot_subdir_bivariate_ap_osc, exist_ok=True)\n",
    "\n",
    "    all_bivariate_ap_pkg_results = []\n",
    "    all_partial_ap_pkg_results = []\n",
    "    all_bivariate_ap_osc_results = []\n",
    "    \n",
    "    # Lists to collect all p-values for FDR correction\n",
    "    all_p_values = []\n",
    "    p_value_mapping = []  # To track which p-value belongs to which test\n",
    "\n",
    "    for channel_label in ORDERED_CHANNEL_LABELS:\n",
    "        df_channel = master_df_step4_processed_c5[master_df_step4_processed_c5[CHANNEL_DISPLAY_COL] == channel_label]\n",
    "        if df_channel.empty:\n",
    "            continue\n",
    "        print(f\"\\\\nProcessing Channel: {channel_label}\")\n",
    "\n",
    "        for freq_label in ORDERED_FREQ_LABELS:\n",
    "            df_channel_freq = df_channel[df_channel[FOOOF_FREQ_BAND_COL] == freq_label].copy()\n",
    "            if df_channel_freq.empty:\n",
    "                continue\n",
    "            print(f\"  Processing Freq Band: {freq_label}\")\n",
    "\n",
    "            # --- Part 5.1: Bivariate Spearman Correlations (Aperiodic vs. PKG) ---\n",
    "            print(f\"    Part 5.1: Bivariate Aperiodic vs. PKG Correlations for {channel_label} ({freq_label})\")\n",
    "            for ap_col, ap_name in APERIODIC_METRICS_COLS.items():\n",
    "                for pkg_col, pkg_name in PKG_METRICS_COLS.items():\n",
    "                    if pkg_col not in df_channel_freq.columns or ap_col not in df_channel_freq.columns:\n",
    "                        continue\n",
    "                    # CHANGE: NO LONGER EXCLUDING ZEROS - using all valid data\n",
    "                    df_granular_for_corr_pkg = df_channel_freq.dropna(subset=[ap_col, pkg_col])\n",
    "                    # Calculate Spearman's correlation\n",
    "                    rho, p_val, N = calculate_spearman_with_n(df_granular_for_corr_pkg, ap_col, pkg_col)\n",
    "\n",
    "                    result_dict = {\n",
    "                        'Channel': channel_label, 'FreqBand': freq_label, \n",
    "                        'AperiodicMetric': ap_name, 'PKGMetric': pkg_name, \n",
    "                        'SpearmanRho': rho, 'PValue': p_val, 'N': N,\n",
    "                        'TestType': 'Bivariate_AP_PKG'\n",
    "                    }\n",
    "                    all_bivariate_ap_pkg_results.append(result_dict)\n",
    "                    \n",
    "                    # Collect p-value for FDR correction\n",
    "                    if not pd.isna(p_val):\n",
    "                        all_p_values.append(p_val)\n",
    "                        p_value_mapping.append(('bivar_ap_pkg', len(all_bivariate_ap_pkg_results) - 1))\n",
    "\n",
    "            # --- Part 5.2: Partial Aperiodic vs. PKG Correlations ---\n",
    "            print(f\"    Part 5.2: Partial Aperiodic vs. PKG Correlations for {channel_label} ({freq_label})\")\n",
    "            covariates_partial_corr = [col for col in OSCILLATORY_METRICS_COLS.keys() if col in df_channel_freq.columns]\n",
    "            if len(covariates_partial_corr) == 2:\n",
    "                for ap_col, ap_name in APERIODIC_METRICS_COLS.items():\n",
    "                    for pkg_col, pkg_name in PKG_METRICS_COLS.items():\n",
    "                        if pkg_col not in df_channel_freq.columns or ap_col not in df_channel_freq.columns:\n",
    "                            continue                        \n",
    "                        cols_for_partial_corr = [ap_col, pkg_col] + covariates_partial_corr\n",
    "                        cols_for_partial_corr_present = [c for c in cols_for_partial_corr if c in df_channel_freq.columns]\n",
    "                        \n",
    "                        if len(cols_for_partial_corr_present) != len(cols_for_partial_corr):\n",
    "                            partial_rho, partial_p_val, N_partial = np.nan, np.nan, 0\n",
    "                        else:\n",
    "                            # CHANGE: NO LONGER EXCLUDING ZEROS\n",
    "                            data_for_partial = df_channel_freq[cols_for_partial_corr_present].dropna()\n",
    "                            if len(data_for_partial) < MIN_SAMPLES_FOR_CORR or not all(data_for_partial[c].nunique() > 1 for c in [ap_col, pkg_col] if c in data_for_partial):\n",
    "                                 partial_rho, partial_p_val, N_partial = np.nan, np.nan, len(data_for_partial)\n",
    "                            else:\n",
    "                                 partial_rho, partial_p_val, N_partial = calculate_partial_spearman(\n",
    "                                     data_for_partial, ap_col, pkg_col, covariates_partial_corr\n",
    "                                 )\n",
    "                        \n",
    "                        result_dict = {\n",
    "                            'Channel': channel_label, 'FreqBand': freq_label,\n",
    "                            'AperiodicMetric': ap_name, 'PKGMetric': pkg_name,\n",
    "                            'PartialSpearmanRho_vs_BetaGamma': partial_rho, 'PartialPValue': partial_p_val, \n",
    "                            'N_Partial': N_partial, 'TestType': 'Partial_AP_PKG'\n",
    "                        }\n",
    "                        all_partial_ap_pkg_results.append(result_dict)\n",
    "                        \n",
    "                        # Collect p-value for FDR correction\n",
    "                        if not pd.isna(partial_p_val):\n",
    "                            all_p_values.append(partial_p_val)\n",
    "                            p_value_mapping.append(('partial_ap_pkg', len(all_partial_ap_pkg_results) - 1))\n",
    "\n",
    "            # --- Part 5.3: Bivariate Aperiodic vs. Oscillatory Correlations ---\n",
    "            print(f\"    Part 5.3: Bivariate Aperiodic vs. Oscillatory Correlations for {channel_label} ({freq_label})\")\n",
    "            for ap_col, ap_name in APERIODIC_METRICS_COLS.items():\n",
    "                for osc_col, osc_name in OSCILLATORY_METRICS_COLS.items():\n",
    "                    if osc_col not in df_channel_freq.columns or ap_col not in df_channel_freq.columns:\n",
    "                        continue\n",
    "                    \n",
    "                    df_granular_for_corr_osc = df_channel_freq.dropna(subset=[ap_col, osc_col])\n",
    "                    rho_ap_osc, p_val_ap_osc, N_ap_osc = calculate_spearman_with_n(df_granular_for_corr_osc, ap_col, osc_col)\n",
    "                    \n",
    "                    result_dict = {\n",
    "                        'Channel': channel_label, 'FreqBand': freq_label, \n",
    "                        'AperiodicMetric': ap_name, 'OscillatoryMetric': osc_name, \n",
    "                        'SpearmanRho': rho_ap_osc, 'PValue': p_val_ap_osc, 'N': N_ap_osc,\n",
    "                        'TestType': 'Bivariate_AP_Osc'\n",
    "                    }\n",
    "                    all_bivariate_ap_osc_results.append(result_dict)\n",
    "                    \n",
    "                    # Collect p-value for FDR correction\n",
    "                    if not pd.isna(p_val_ap_osc):\n",
    "                        all_p_values.append(p_val_ap_osc)\n",
    "                        p_value_mapping.append(('bivar_ap_osc', len(all_bivariate_ap_osc_results) - 1))\n",
    "\n",
    "    # --- Apply FDR Correction to all p-values ---\n",
    "    print(f\"\\\\nCollected {len(all_p_values)} p-values for FDR correction.\")\n",
    "    \n",
    "    if all_p_values:\n",
    "        # Apply FDR correction\n",
    "        rejected, pvals_corrected = fdrcorrection(all_p_values, alpha=0.05, method='indep', is_sorted=False)\n",
    "        \n",
    "        # Map corrected p-values back to results\n",
    "        for idx, (test_type, result_idx) in enumerate(p_value_mapping):\n",
    "            if test_type == 'bivar_ap_pkg':\n",
    "                all_bivariate_ap_pkg_results[result_idx]['PValue_FDR'] = pvals_corrected[idx]\n",
    "                all_bivariate_ap_pkg_results[result_idx]['Significant_FDR'] = rejected[idx]\n",
    "            elif test_type == 'partial_ap_pkg':\n",
    "                all_partial_ap_pkg_results[result_idx]['PartialPValue_FDR'] = pvals_corrected[idx]\n",
    "                all_partial_ap_pkg_results[result_idx]['Significant_FDR'] = rejected[idx]\n",
    "            elif test_type == 'bivar_ap_osc':\n",
    "                all_bivariate_ap_osc_results[result_idx]['PValue_FDR'] = pvals_corrected[idx]\n",
    "                all_bivariate_ap_osc_results[result_idx]['Significant_FDR'] = rejected[idx]\n",
    "        \n",
    "        print(\"FDR correction applied successfully.\")\n",
    "    \n",
    "    # --- Generate plots only for tests that remain significant after FDR ---\n",
    "    for i, result in enumerate(all_bivariate_ap_pkg_results):\n",
    "        if result['N'] >= MIN_SAMPLES_FOR_CORR and result.get('Significant_FDR', False):\n",
    "            # Recreate the data for plotting\n",
    "            channel_label = result['Channel']\n",
    "            freq_label = result['FreqBand']\n",
    "            ap_name = result['AperiodicMetric']\n",
    "            pkg_name = result['PKGMetric']\n",
    "            ap_col = [k for k, v in APERIODIC_METRICS_COLS.items() if v == ap_name][0]\n",
    "            pkg_col = [k for k, v in PKG_METRICS_COLS.items() if v == pkg_name][0]\n",
    "            \n",
    "            df_plot = master_df_step4_processed_c5[\n",
    "                (master_df_step4_processed_c5[CHANNEL_DISPLAY_COL] == channel_label) &\n",
    "                (master_df_step4_processed_c5[FOOOF_FREQ_BAND_COL] == freq_label)\n",
    "            ].dropna(subset=[ap_col, pkg_col])\n",
    "            \n",
    "            if not df_plot.empty:\n",
    "                plt.figure(figsize=(6, 6))\n",
    "                ax = plt.gca()\n",
    "                ax.grid(False)\n",
    "\n",
    "                df_averaged_points_pkg = df_plot.set_index('datetime_for_avg_c5').groupby(pd.Grouper(freq='10T'))[[ap_col, pkg_col]].mean().dropna() if 'datetime_for_avg_c5' in df_plot.columns and not df_plot['datetime_for_avg_c5'].isnull().all() else df_plot.copy()\n",
    "\n",
    "                if not df_averaged_points_pkg.empty:\n",
    "                     sns.scatterplot(data=df_averaged_points_pkg, x=pkg_col, y=ap_col, color=MODIFIED_COLOR_PALETTE.get(ap_col, 'grey'), alpha=DOT_ALPHA_STEP4 + 0.1, s=40, edgecolor='k', linewidths=0.5, ax=ax)\n",
    "                sns.regplot(data=df_plot, x=pkg_col, y=ap_col, scatter=False, ax=ax, line_kws={'color': MODIFIED_COLOR_PALETTE.get(pkg_col, 'black'), 'linewidth': REG_LINE_THICKNESS_STEP4, 'alpha': 0.7})\n",
    "\n",
    "                # Use FDR-corrected p-value for annotation\n",
    "                annotate_correlation_on_plot(ax, result['SpearmanRho'], result['PValue_FDR'], result['N'], \n",
    "                                           test_type=\"Spearman ρ (FDR)\", fontsize=9 * font_scale_factor)\n",
    "                \n",
    "                simple_pkg_name = pkg_name.replace('PKG ', '')\n",
    "                simple_ap_name = ap_name.replace('Aperiodic ', '')\n",
    "                ax.set_xlabel(simple_pkg_name, fontsize=plt.rcParams['axes.labelsize'] * font_scale_factor)\n",
    "                ax.set_ylabel(simple_ap_name, fontsize=plt.rcParams['axes.labelsize'] * font_scale_factor)\n",
    "                ax.tick_params(axis='both', which='major', labelsize=plt.rcParams['xtick.labelsize'] * font_scale_factor)\n",
    "                \n",
    "                # Set hardcoded ticks and limits\n",
    "                y_ticks = [0, 1, 2, 3, 4, 5]\n",
    "                x_ticks = [0, 20, 40, 60, 80]\n",
    "                ax.set_yticks(y_ticks)\n",
    "                ax.set_xticks(x_ticks)\n",
    "                ax.set_ylim(0, 5)\n",
    "                ax.set_xlim(0, 80)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                safe_ch, safe_ap, safe_pkg = get_safe_filename_step4(channel_label), get_safe_filename_step4(ap_name), get_safe_filename_step4(pkg_name)\n",
    "                plot_filename = f\"Bivar_{safe_ap}_vs_{safe_pkg}_{safe_ch}_{freq_label}_FDRsig.png\"\n",
    "                plt.savefig(os.path.join(plot_subdir_bivariate_ap_pkg, plot_filename))\n",
    "                plt.close()\n",
    "\n",
    "    # Similar plotting for oscillatory correlations (only FDR significant)\n",
    "    for i, result in enumerate(all_bivariate_ap_osc_results):\n",
    "        if result['N'] >= MIN_SAMPLES_FOR_CORR and result.get('Significant_FDR', False):\n",
    "            # Similar plotting code for oscillatory...\n",
    "            # [Code abbreviated for brevity - follows same pattern as above]\n",
    "            pass\n",
    "\n",
    "    # --- CSV Saving Logic ---\n",
    "    if all_bivariate_ap_pkg_results:\n",
    "        df_bivar_ap_pkg = pd.DataFrame(all_bivariate_ap_pkg_results)\n",
    "        df_bivar_ap_pkg.to_csv(os.path.join(analysis_session_plot_folder_step4, f\"{patient_hemisphere_id}_Bivariate_AP_vs_PKG_Correlations_AllData_FDR_Cell5.csv\"), index=False)\n",
    "        print(f\"\\\\nSaved Bivariate AP vs PKG correlation results (all data, FDR corrected) for {patient_hemisphere_id}.\")\n",
    "\n",
    "    if all_partial_ap_pkg_results:\n",
    "        df_partial_ap_pkg = pd.DataFrame(all_partial_ap_pkg_results)\n",
    "        df_partial_ap_pkg.to_csv(os.path.join(analysis_session_plot_folder_step4, f\"{patient_hemisphere_id}_Partial_AP_vs_PKG_Correlations_AllData_FDR_Cell5.csv\"), index=False)\n",
    "        print(f\"Saved Partial AP vs PKG correlation results (all data, FDR corrected) for {patient_hemisphere_id}.\")\n",
    "\n",
    "    if all_bivariate_ap_osc_results:\n",
    "        df_bivar_ap_osc = pd.DataFrame(all_bivariate_ap_osc_results)\n",
    "        df_bivar_ap_osc.to_csv(os.path.join(analysis_session_plot_folder_step4, f\"{patient_hemisphere_id}_Bivariate_AP_vs_Oscillatory_Correlations_FDR_Cell5.csv\"), index=False)\n",
    "        print(f\"Saved Bivariate AP vs Oscillatory correlation results (FDR corrected) for {patient_hemisphere_id}.\")\n",
    "\n",
    "print(\"\\\\n--- Cell 5 (Revised V6 - All Data + FDR): Correlation Analyses Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74691efc-4f96-42ce-a155-9d299bad954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 6 (Streamlined V4 - Tiered MLR with LRT): Multiple Linear Regression ---\n",
    "# Focuses on model comparison using Likelihood Ratio Test (LRT) for nested models.\n",
    "# Interpretation output is cleaned, and AIC/BIC are included.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "print(\"\\n--- Cell 6 (Streamlined V4 - Tiered MLR with LRT): Starting Analyses ---\\n\")\n",
    "\n",
    "P_VALUE_THRESHOLD = 0.05\n",
    "print(f\"p-value is now defined as {P_VALUE_THRESHOLD}.\")\n",
    "\n",
    "# <<< --- USER TOGGLE --- >>>\n",
    "ANALYZE_ALL_FREQ_BANDS = False\n",
    "TARGET_FREQ_BAND_IF_NOT_ALL = \"WideFreq\"\n",
    "# <<< ------------------ >>>\n",
    "\n",
    "if 'master_df_step4' not in locals() or master_df_step4.empty:\n",
    "    print(\"master_df_step4 not available or empty. Skipping Cell 6.\")\n",
    "else:\n",
    "    plot_subdir_mlr = os.path.join(analysis_session_plot_folder_step4, \"MultipleLinearRegression_PKG_on_Neural_STREAMLINED_V4\") # New folder name\n",
    "    os.makedirs(plot_subdir_mlr, exist_ok=True)\n",
    "    \n",
    "    plot_subdir_ap_corr = os.path.join(plot_subdir_mlr, \"Aperiodic_Intercorrelations\")\n",
    "    os.makedirs(plot_subdir_ap_corr, exist_ok=True)\n",
    "    \n",
    "    mlr_results_list_streamlined_v4 = []\n",
    "    lrt_results_list = [] # New list to store LRT results\n",
    "    exp_offset_corr_results_list = []\n",
    "\n",
    "    available_aperiodic_cols = [col for col in APERIODIC_METRICS_COLS.keys() if col in master_df_step4.columns]\n",
    "    available_oscillatory_cols = [col for col in OSCILLATORY_METRICS_COLS.keys() if col in master_df_step4.columns]\n",
    "\n",
    "    exponent_col_name = 'Exponent_BestModel'\n",
    "    offset_col_name = 'Offset_BestModel'\n",
    "    beta_col_name = 'Beta_Peak_Power_at_DominantFreq'\n",
    "    gamma_col_name = 'Gamma_Peak_Power_at_DominantFreq'\n",
    "\n",
    "    # Helper function modified to return the model fit and include AIC/BIC.\n",
    "    # (In Cell 6) Replace the old helper function with this one.\n",
    "    def fit_and_interpret_mlr_focused_v4(data_df, dv_col, dv_name, predictors, model_tier_label,\n",
    "                                         channel=\"N/A\", freq_band=\"N/A\"):\n",
    "        \n",
    "        if not predictors or not all(p in data_df.columns for p in predictors):\n",
    "            missing_p = [p for p in predictors if p not in data_df.columns] if predictors else []\n",
    "            if predictors and missing_p:\n",
    "                print(f\"      Skipping {dv_name} ({model_tier_label}): Predictor(s) {missing_p} not found.\")\n",
    "            elif not predictors:\n",
    "                print(f\"      Skipping {dv_name} ({model_tier_label}): No valid predictors for this model tier.\")\n",
    "            return None\n",
    "    \n",
    "        formula_to_fit = f\"{dv_col} ~ {' + '.join(predictors)}\"\n",
    "        current_data_for_model = data_df[[dv_col] + predictors].dropna(how='any').copy()\n",
    "    \n",
    "        if len(current_data_for_model) < (len(predictors) + 10):\n",
    "            print(f\"      Skipping {dv_name} ({model_tier_label}): Insufficient data ({len(current_data_for_model)}) for {len(predictors)} predictors.\")\n",
    "            return None\n",
    "    \n",
    "        for pred_check in predictors:\n",
    "            if current_data_for_model[pred_check].nunique() < 2:\n",
    "                print(f\"      Skipping {dv_name} ({model_tier_label}): Constant predictor {pred_check} found.\")\n",
    "                return None\n",
    "        \n",
    "        print(f\"      --- Model Tier: {model_tier_label} ---\")\n",
    "        print(f\"      Fitting for DV '{dv_name}' with formula: {formula_to_fit}\")\n",
    "        \n",
    "        try:\n",
    "            model_fit = smf.ols(formula=formula_to_fit, data=current_data_for_model).fit()\n",
    "            \n",
    "            interpretation_string = f\"      --- Interpretation for {dv_name} ({model_tier_label} - {channel}, {freq_band}) ---\\n\"\n",
    "            interpretation_string += f\"      Overall Model Fit: Explains {model_fit.rsquared_adj * 100:.1f}% of variance (Adj. R² = {model_fit.rsquared_adj:.3f}, N = {model_fit.nobs}).\\n\"\n",
    "            interpretation_string += f\"      Model Selection Criteria: AIC = {model_fit.aic:.2f}, BIC = {model_fit.bic:.2f}.\\n\"\n",
    "            print(interpretation_string) # Print interpretation right away\n",
    "            \n",
    "            for term_in_model in model_fit.params.index:\n",
    "                if term_in_model == 'Intercept': continue\n",
    "                \n",
    "                # This is the dictionary that gets appended to the results list\n",
    "                mlr_results_list_streamlined_v4.append({\n",
    "                    'Channel': channel, 'FreqBand_Aperiodics': freq_band, 'PKG_Symptom_DV': dv_name,\n",
    "                    'Model_Tier': model_tier_label, 'Formula': formula_to_fit,\n",
    "                    'Predictor_Term': term_in_model,\n",
    "                    'Predictor_Name_Display': APERIODIC_METRICS_COLS.get(term_in_model, OSCILLATORY_METRICS_COLS.get(term_in_model, term_in_model)),\n",
    "                    'Coefficient': model_fit.params.get(term_in_model, np.nan),\n",
    "                    'StdErr': model_fit.bse.get(term_in_model, np.nan),\n",
    "                    'PValue': model_fit.pvalues.get(term_in_model, np.nan),\n",
    "                    # === ADDED CONFIDENCE INTERVALS ===\n",
    "                    'Conf_Int_Lower': model_fit.conf_int().loc[term_in_model, 0] if term_in_model in model_fit.conf_int().index else np.nan,\n",
    "                    'Conf_Int_Upper': model_fit.conf_int().loc[term_in_model, 1] if term_in_model in model_fit.conf_int().index else np.nan,\n",
    "                    # ==================================\n",
    "                    'N_model': model_fit.nobs, 'R_squared_adj_model': model_fit.rsquared_adj,\n",
    "                    'AIC_model': model_fit.aic, 'BIC_model': model_fit.bic\n",
    "                })\n",
    "            \n",
    "            print(f\"      --- End of Interpretation for {model_tier_label} ---\\n\")\n",
    "            return model_fit\n",
    "            \n",
    "        except Exception as e_mlr_fit:\n",
    "            print(f\"      ERROR fitting MLR for {dv_name} ({model_tier_label}): {e_mlr_fit}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    freq_bands_to_process = [TARGET_FREQ_BAND_IF_NOT_ALL] if not ANALYZE_ALL_FREQ_BANDS else ORDERED_FREQ_LABELS\n",
    "    if not ANALYZE_ALL_FREQ_BANDS: print(f\"--- Analyzing ONLY for Freq Band: {TARGET_FREQ_BAND_IF_NOT_ALL} ---\")\n",
    "\n",
    "    for channel_label_iter in ORDERED_CHANNEL_LABELS:\n",
    "        df_channel_mlr_main = master_df_step4[master_df_step4[CHANNEL_DISPLAY_COL] == channel_label_iter]\n",
    "        if df_channel_mlr_main.empty: continue\n",
    "        print(f\"\\n>>> Processing MLR for Channel: {channel_label_iter} <<<\\n\")\n",
    "\n",
    "        for freq_label_iter in freq_bands_to_process:\n",
    "            df_channel_freq_mlr_main = df_channel_mlr_main[df_channel_mlr_main[FOOOF_FREQ_BAND_COL] == freq_label_iter].copy()\n",
    "            if df_channel_freq_mlr_main.empty: continue\n",
    "            print(f\"  --- Freq Band: {freq_label_iter} ---\\n\")\n",
    "\n",
    "            # Aperiodic inter-correlation check (same as before)\n",
    "            if exponent_col_name in df_channel_freq_mlr_main.columns and offset_col_name in df_channel_freq_mlr_main.columns:\n",
    "                # This part of the code for correlation plotting remains the same...\n",
    "                # (Code for correlation check and plotting is omitted for brevity but should be kept from your original script)\n",
    "                pass\n",
    "\n",
    "            for pkg_col_iter, pkg_name_iter in PKG_METRICS_COLS.items():\n",
    "                if pkg_col_iter not in df_channel_freq_mlr_main.columns: continue\n",
    "                print(f\"    --- Predicting: {pkg_name_iter} (DV: {pkg_col_iter}) ---\\n\")\n",
    "\n",
    "                # --- NEW STRUCTURE FOR LRT ---\n",
    "                # These variables will hold the fitted models for comparison\n",
    "                osc_only_fit = None\n",
    "                exp_osc_fit = None\n",
    "                off_osc_fit = None\n",
    "                \n",
    "                # --- FIT BASELINE MODELS ---\n",
    "                # Fit Oscillatory Only model (this is our primary reduced model)\n",
    "                osc_predictors = [p for p in [beta_col_name, gamma_col_name] if p in available_oscillatory_cols and p in df_channel_freq_mlr_main.columns]\n",
    "                if len(osc_predictors) > 0:\n",
    "                    osc_only_fit = fit_and_interpret_mlr_focused_v4(df_channel_freq_mlr_main, pkg_col_iter, pkg_name_iter,\n",
    "                                                                    osc_predictors, \"Tier 1: Oscillatory Only\",\n",
    "                                                                    channel=channel_label_iter, freq_band=freq_label_iter)\n",
    "\n",
    "                # --- FIT FULL MODELS AND PERFORM LRT ---\n",
    "                # Test adding Exponent\n",
    "                if osc_only_fit and exponent_col_name in available_aperiodic_cols:\n",
    "                    exp_osc_predictors = [exponent_col_name] + osc_predictors\n",
    "                    exp_osc_fit = fit_and_interpret_mlr_focused_v4(df_channel_freq_mlr_main, pkg_col_iter, pkg_name_iter,\n",
    "                                                                   exp_osc_predictors, \"Tier 2: Exponent + Oscillatory\",\n",
    "                                                                   channel=channel_label_iter, freq_band=freq_label_iter)\n",
    "                    # Perform LRT if both models were fit successfully\n",
    "                    if exp_osc_fit:\n",
    "                        lrt_results_exp = anova_lm(osc_only_fit, exp_osc_fit)\n",
    "                        p_value_lrt = lrt_results_exp.iloc[1]['Pr(>F)']\n",
    "                        f_stat_lrt = lrt_results_exp.iloc[1]['F']\n",
    "                        \n",
    "                        lrt_interpretation = f\"      --- Likelihood Ratio Test (LRT) for Exponent ---\\n\"\n",
    "                        lrt_interpretation += f\"      Comparing 'Oscillatory Only' vs 'Exponent + Oscillatory' for DV '{pkg_name_iter}'.\\n\"\n",
    "                        lrt_interpretation += f\"      LRT Result: F-statistic = {f_stat_lrt:.3f}, p-value = {p_value_lrt:.4g}\\n\"\n",
    "                        if p_value_lrt < P_VALUE_THRESHOLD:\n",
    "                            lrt_interpretation += f\"      Interpretation: Adding Exponent resulted in a SIGNIFICANT improvement in model fit (p < {P_VALUE_THRESHOLD}).\\n\"\n",
    "                        else:\n",
    "                            lrt_interpretation += f\"      Interpretation: Adding Exponent did NOT significantly improve model fit (p >= {P_VALUE_THRESHOLD}).\\n\"\n",
    "                        print(lrt_interpretation)\n",
    "                        \n",
    "                        lrt_results_list.append({\n",
    "                            'Channel': channel_label_iter, 'FreqBand': freq_label_iter, 'PKG_Symptom_DV': pkg_name_iter,\n",
    "                            'Comparison': 'Exponent + Osc vs. Osc Only',\n",
    "                            'F_statistic': f_stat_lrt, 'P_value': p_value_lrt, 'N_reduced': osc_only_fit.nobs, 'N_full': exp_osc_fit.nobs\n",
    "                        })\n",
    "                \n",
    "                # Test adding Offset\n",
    "                if osc_only_fit and offset_col_name in available_aperiodic_cols:\n",
    "                    off_osc_predictors = [offset_col_name] + osc_predictors\n",
    "                    off_osc_fit = fit_and_interpret_mlr_focused_v4(df_channel_freq_mlr_main, pkg_col_iter, pkg_name_iter,\n",
    "                                                                   off_osc_predictors, \"Tier 3: Offset + Oscillatory\",\n",
    "                                                                   channel=channel_label_iter, freq_band=freq_label_iter)\n",
    "                    # Perform LRT if both models were fit successfully\n",
    "                    if off_osc_fit:\n",
    "                        lrt_results_off = anova_lm(osc_only_fit, off_osc_fit)\n",
    "                        p_value_lrt = lrt_results_off.iloc[1]['Pr(>F)']\n",
    "                        f_stat_lrt = lrt_results_off.iloc[1]['F']\n",
    "                        \n",
    "                        lrt_interpretation = f\"      --- Likelihood Ratio Test (LRT) for Offset ---\\n\"\n",
    "                        lrt_interpretation += f\"      Comparing 'Oscillatory Only' vs 'Offset + Oscillatory' for DV '{pkg_name_iter}'.\\n\"\n",
    "                        lrt_interpretation += f\"      LRT Result: F-statistic = {f_stat_lrt:.3f}, p-value = {p_value_lrt:.4g}\\n\"\n",
    "                        if p_value_lrt < P_VALUE_THRESHOLD:\n",
    "                            lrt_interpretation += f\"      Interpretation: Adding Offset resulted in a SIGNIFICANT improvement in model fit (p < {P_VALUE_THRESHOLD}).\\n\"\n",
    "                        else:\n",
    "                            lrt_interpretation += f\"      Interpretation: Adding Offset did NOT significantly improve model fit (p >= {P_VALUE_THRESHOLD}).\\n\"\n",
    "                        print(lrt_interpretation)\n",
    "\n",
    "                        lrt_results_list.append({\n",
    "                            'Channel': channel_label_iter, 'FreqBand': freq_label_iter, 'PKG_Symptom_DV': pkg_name_iter,\n",
    "                            'Comparison': 'Offset + Osc vs. Osc Only',\n",
    "                            'F_statistic': f_stat_lrt, 'P_value': p_value_lrt, 'N_reduced': osc_only_fit.nobs, 'N_full': off_osc_fit.nobs\n",
    "                        })\n",
    "\n",
    "\n",
    "    # Saving all results to CSV files at the end\n",
    "\n",
    "    if lrt_results_list:\n",
    "        df_lrt_results = pd.DataFrame(lrt_results_list)\n",
    "        csv_filename_lrt = f\"{patient_hemisphere_id}_MLR_LRT_Results_Step6.csv\"\n",
    "        df_lrt_results.to_csv(os.path.join(plot_subdir_mlr, csv_filename_lrt), index=False)\n",
    "        print(f\"\\nSaved Likelihood Ratio Test (LRT) results to {csv_filename_lrt}.\")\n",
    "        print(\"Sample of LRT results:\")\n",
    "        print(df_lrt_results.head())\n",
    "    else:\n",
    "        print(\"\\nNo Likelihood Ratio Tests were performed.\")\n",
    "\n",
    "    if mlr_results_list_streamlined_v4:\n",
    "        df_mlr_results_step6_streamlined_v4 = pd.DataFrame(mlr_results_list_streamlined_v4)\n",
    "        csv_filename_mlr_streamlined_v4 = f\"{patient_hemisphere_id}_MLR_Streamlined_V4_Results_Step6.csv\"\n",
    "        df_mlr_results_step6_streamlined_v4.to_csv(os.path.join(plot_subdir_mlr, csv_filename_mlr_streamlined_v4), index=False)\n",
    "        print(f\"\\nSaved Step 6 Streamlined V4 MLR results for {patient_hemisphere_id} to {csv_filename_mlr_streamlined_v4}.\")\n",
    "        cols_to_show_mlr = ['Channel', 'FreqBand_Aperiodics', 'PKG_Symptom_DV', 'Model_Tier',\n",
    "                            'Predictor_Term', 'Coefficient', 'PValue', 'R_squared_adj_model', 'N_model']\n",
    "        print(df_mlr_results_step6_streamlined_v4[[c for c in cols_to_show_mlr if c in df_mlr_results_step6_streamlined_v4.columns]].head())\n",
    "    else:\n",
    "        print(\"\\nNo Streamlined V4 MLR models were successfully fitted or no results to save for Step 6.\")\n",
    "\n",
    "print(f\"\\n--- Cell 6 (Streamlined V4 - Tiered MLR with LRT): Analyses Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec00f937-a0d2-4816-b5f3-bf30547aa712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 6A (Revised): State-Specific Multiple Linear Regression with LRT + FDR ---\n",
    "# Changes:\n",
    "# 1. ADDED FDR correction for all LRT p-values\n",
    "# 2. All data is kept (no exclusion of zeros)\n",
    "# Repeats the tiered MLR and Likelihood Ratio Test analyses for each of the\n",
    "# TARGET_CLINICAL_STATES_ORDERED to see if relationships change with clinical state.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "print(\"\\\\n--- Cell 6A (Revised): Starting State-Specific MLR & LRT Analyses with FDR ---\\\\n\")\n",
    "\n",
    "# <<< --- USER TOGGLE --- >>>\n",
    "# This analysis is intensive. It is recommended to run it on a single frequency band first.\n",
    "ANALYZE_ALL_FREQ_BANDS_STATE_SPECIFIC = False\n",
    "TARGET_FREQ_BAND_IF_NOT_ALL_STATE_SPECIFIC = \"WideFreq\"\n",
    "# <<< ------------------ >>>\n",
    "\n",
    "if 'master_df_step4' not in locals() or master_df_step4.empty:\n",
    "    print(\"master_df_step4 not available or empty. Skipping Cell 6A.\")\n",
    "else:\n",
    "    # Define a new directory for this state-specific analysis output\n",
    "    plot_subdir_mlr_states = os.path.join(STATE_SPECIFIC_ANALYSIS_DIR, \"MultipleLinearRegression_State_Specific_FDR\")\n",
    "    os.makedirs(plot_subdir_mlr_states, exist_ok=True)\n",
    "    \n",
    "    # New lists to hold the results from all states\n",
    "    mlr_results_list_state_specific = []\n",
    "    lrt_results_list_state_specific = []\n",
    "    \n",
    "    # Lists to collect all p-values for FDR\n",
    "    all_lrt_p_values_states = []\n",
    "    lrt_p_value_mapping_states = []\n",
    "\n",
    "    # Define variable names for clarity\n",
    "    available_aperiodic_cols = [col for col in APERIODIC_METRICS_COLS.keys() if col in master_df_step4.columns]\n",
    "    available_oscillatory_cols = [col for col in OSCILLATORY_METRICS_COLS.keys() if col in master_df_step4.columns]\n",
    "    exponent_col_name = 'Exponent_BestModel'\n",
    "    offset_col_name = 'Offset_BestModel'\n",
    "    beta_col_name = 'Beta_Peak_Power_at_DominantFreq'\n",
    "    gamma_col_name = 'Gamma_Peak_Power_at_DominantFreq'\n",
    "\n",
    "    # Helper function is modified to accept 'clinical_state' for inclusion in results\n",
    "    def fit_and_interpret_mlr_state_specific(data_df, dv_col, dv_name, predictors, model_tier_label,\n",
    "                                             channel=\"N/A\", freq_band=\"N/A\", clinical_state=\"N/A\"):\n",
    "        \n",
    "        if not predictors or not all(p in data_df.columns for p in predictors):\n",
    "            return None\n",
    "    \n",
    "        formula_to_fit = f\"{dv_col} ~ {' + '.join(predictors)}\"\n",
    "        current_data_for_model = data_df[[dv_col] + predictors].dropna(how='any').copy()\n",
    "    \n",
    "        if len(current_data_for_model) < (len(predictors) + 10):\n",
    "            return None\n",
    "    \n",
    "        for pred_check in predictors:\n",
    "            if current_data_for_model[pred_check].nunique() < 2:\n",
    "                return None\n",
    "        \n",
    "        try:\n",
    "            model_fit = smf.ols(formula=formula_to_fit, data=current_data_for_model).fit()\n",
    "            \n",
    "            for term_in_model in model_fit.params.index:\n",
    "                if term_in_model == 'Intercept': continue\n",
    "                \n",
    "                mlr_results_list_state_specific.append({\n",
    "                    'ClinicalState': clinical_state,\n",
    "                    'Channel': channel, 'FreqBand_Aperiodics': freq_band, 'PKG_Symptom_DV': dv_name,\n",
    "                    'Model_Tier': model_tier_label, 'Formula': formula_to_fit,\n",
    "                    'Predictor_Term': term_in_model,\n",
    "                    'Predictor_Name_Display': APERIODIC_METRICS_COLS.get(term_in_model, OSCILLATORY_METRICS_COLS.get(term_in_model, term_in_model)),\n",
    "                    'Coefficient': model_fit.params.get(term_in_model, np.nan),\n",
    "                    'StdErr': model_fit.bse.get(term_in_model, np.nan),\n",
    "                    'PValue': model_fit.pvalues.get(term_in_model, np.nan),\n",
    "                    'Conf_Int_Lower': model_fit.conf_int().loc[term_in_model, 0] if term_in_model in model_fit.conf_int().index else np.nan,\n",
    "                    'Conf_Int_Upper': model_fit.conf_int().loc[term_in_model, 1] if term_in_model in model_fit.conf_int().index else np.nan,\n",
    "                    'N_model': model_fit.nobs, 'R_squared_adj_model': model_fit.rsquared_adj,\n",
    "                    'AIC_model': model_fit.aic, 'BIC_model': model_fit.bic\n",
    "                })\n",
    "            return model_fit\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    freq_bands_to_process = [TARGET_FREQ_BAND_IF_NOT_ALL_STATE_SPECIFIC] if not ANALYZE_ALL_FREQ_BANDS_STATE_SPECIFIC else ORDERED_FREQ_LABELS\n",
    "    if not ANALYZE_ALL_FREQ_BANDS_STATE_SPECIFIC: print(f\"--- Analyzing ONLY for Freq Band: {TARGET_FREQ_BAND_IF_NOT_ALL_STATE_SPECIFIC} ---\")\n",
    "\n",
    "    # --- Outermost loop for Clinical States ---\n",
    "    for state_current in TARGET_CLINICAL_STATES_ORDERED:\n",
    "        df_state = master_df_step4[master_df_step4[CLINICAL_STATE_COL] == state_current]\n",
    "        \n",
    "        if df_state.empty or len(df_state) < 20: # Setting a reasonable minimum N for a state\n",
    "            print(f\"\\\\nSKIPPING Clinical State: {state_current} (Insufficient data: N={len(df_state)})\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\\\n>>> Processing MLR for Clinical State: {state_current} (N={len(df_state)}) <<<\")\n",
    "\n",
    "        for channel_label_iter in ORDERED_CHANNEL_LABELS:\n",
    "            df_channel_state = df_state[df_state[CHANNEL_DISPLAY_COL] == channel_label_iter]\n",
    "            if df_channel_state.empty: continue\n",
    "            print(f\"\\\\n  >> Channel: {channel_label_iter}\")\n",
    "\n",
    "            for freq_label_iter in freq_bands_to_process:\n",
    "                df_channel_freq_state = df_channel_state[df_channel_state[FOOOF_FREQ_BAND_COL] == freq_label_iter].copy()\n",
    "                if df_channel_freq_state.empty: continue\n",
    "                print(f\"\\\\n    -- Freq Band: {freq_label_iter} --\")\n",
    "\n",
    "                for pkg_col_iter, pkg_name_iter in PKG_METRICS_COLS.items():\n",
    "                    if pkg_col_iter not in df_channel_freq_state.columns: continue\n",
    "                    print(f\"\\\\n      -- Predicting: {pkg_name_iter} (DV: {pkg_col_iter}) --\")\n",
    "\n",
    "                    osc_only_fit, exp_osc_fit, off_osc_fit = None, None, None\n",
    "                    \n",
    "                    osc_predictors = [p for p in [beta_col_name, gamma_col_name] if p in available_oscillatory_cols and p in df_channel_freq_state.columns]\n",
    "                    if len(osc_predictors) > 0:\n",
    "                        osc_only_fit = fit_and_interpret_mlr_state_specific(\n",
    "                            df_channel_freq_state, pkg_col_iter, pkg_name_iter, osc_predictors, \n",
    "                            \"Tier 1: Oscillatory Only\", channel_label_iter, freq_label_iter, state_current)\n",
    "\n",
    "                    if osc_only_fit and exponent_col_name in available_aperiodic_cols:\n",
    "                        exp_osc_predictors = [exponent_col_name] + osc_predictors\n",
    "                        exp_osc_fit = fit_and_interpret_mlr_state_specific(\n",
    "                            df_channel_freq_state, pkg_col_iter, pkg_name_iter, exp_osc_predictors, \n",
    "                            \"Tier 2: Exponent + Oscillatory\", channel_label_iter, freq_label_iter, state_current)\n",
    "                        \n",
    "                        if exp_osc_fit:\n",
    "                            lrt_results_exp = anova_lm(osc_only_fit, exp_osc_fit)\n",
    "                            p_val_lrt = lrt_results_exp.iloc[1]['Pr(>F)']\n",
    "                            f_stat_lrt = lrt_results_exp.iloc[1]['F']\n",
    "                            print(f\"        LRT (Exponent): F={f_stat_lrt:.2f}, p={p_val_lrt:.3g}\")\n",
    "                            \n",
    "                            lrt_result_dict = {\n",
    "                                'ClinicalState': state_current, 'Channel': channel_label_iter, 'FreqBand': freq_label_iter, \n",
    "                                'PKG_Symptom_DV': pkg_name_iter, 'Comparison': 'Exponent + Osc vs. Osc Only',\n",
    "                                'F_statistic': f_stat_lrt, 'P_value': p_val_lrt, 'N_reduced': osc_only_fit.nobs, 'N_full': exp_osc_fit.nobs\n",
    "                            }\n",
    "                            lrt_results_list_state_specific.append(lrt_result_dict)\n",
    "                            \n",
    "                            # Collect p-value for FDR\n",
    "                            if not pd.isna(p_val_lrt):\n",
    "                                all_lrt_p_values_states.append(p_val_lrt)\n",
    "                                lrt_p_value_mapping_states.append(len(lrt_results_list_state_specific) - 1)\n",
    "                    \n",
    "                    if osc_only_fit and offset_col_name in available_aperiodic_cols:\n",
    "                        off_osc_predictors = [offset_col_name] + osc_predictors\n",
    "                        off_osc_fit = fit_and_interpret_mlr_state_specific(\n",
    "                            df_channel_freq_state, pkg_col_iter, pkg_name_iter, off_osc_predictors,\n",
    "                            \"Tier 3: Offset + Oscillatory\", channel_label_iter, freq_label_iter, state_current)\n",
    "                        \n",
    "                        if off_osc_fit:\n",
    "                            lrt_results_off = anova_lm(osc_only_fit, off_osc_fit)\n",
    "                            p_val_lrt = lrt_results_off.iloc[1]['Pr(>F)']\n",
    "                            f_stat_lrt = lrt_results_off.iloc[1]['F']\n",
    "                            print(f\"        LRT (Offset):   F={f_stat_lrt:.2f}, p={p_val_lrt:.3g}\")\n",
    "                            \n",
    "                            lrt_result_dict = {\n",
    "                                'ClinicalState': state_current, 'Channel': channel_label_iter, 'FreqBand': freq_label_iter, \n",
    "                                'PKG_Symptom_DV': pkg_name_iter, 'Comparison': 'Offset + Osc vs. Osc Only',\n",
    "                                'F_statistic': f_stat_lrt, 'P_value': p_val_lrt, 'N_reduced': osc_only_fit.nobs, 'N_full': off_osc_fit.nobs\n",
    "                            }\n",
    "                            lrt_results_list_state_specific.append(lrt_result_dict)\n",
    "                            \n",
    "                            # Collect p-value for FDR\n",
    "                            if not pd.isna(p_val_lrt):\n",
    "                                all_lrt_p_values_states.append(p_val_lrt)\n",
    "                                lrt_p_value_mapping_states.append(len(lrt_results_list_state_specific) - 1)\n",
    "\n",
    "    # Apply FDR correction to all state-specific LRT p-values\n",
    "    print(f\"\\\\nCollected {len(all_lrt_p_values_states)} state-specific LRT p-values for FDR correction.\")\n",
    "    \n",
    "    if all_lrt_p_values_states:\n",
    "        rejected, pvals_corrected = fdrcorrection(all_lrt_p_values_states, alpha=0.05, method='indep', is_sorted=False)\n",
    "        \n",
    "        # Map corrected p-values back to results\n",
    "        for idx, result_idx in enumerate(lrt_p_value_mapping_states):\n",
    "            lrt_results_list_state_specific[result_idx]['P_value_FDR'] = pvals_corrected[idx]\n",
    "            lrt_results_list_state_specific[result_idx]['Significant_FDR'] = rejected[idx]\n",
    "        \n",
    "        print(\"FDR correction applied to state-specific LRT results.\")\n",
    "\n",
    "    # --- Saving all collected state-specific results ---\n",
    "    if lrt_results_list_state_specific:\n",
    "        df_lrt_results_states = pd.DataFrame(lrt_results_list_state_specific)\n",
    "        csv_filename_lrt_states = f\"{patient_hemisphere_id}_MLR_LRT_StateSpecific_Results_FDR_Step6A.csv\"\n",
    "        df_lrt_results_states.to_csv(os.path.join(plot_subdir_mlr_states, csv_filename_lrt_states), index=False)\n",
    "        print(f\"\\\\nSaved State-Specific Likelihood Ratio Test (LRT) results with FDR to {csv_filename_lrt_states}.\")\n",
    "        print(\"Sample of State-Specific LRT results with FDR:\")\n",
    "        print(df_lrt_results_states[['ClinicalState', 'Channel', 'PKG_Symptom_DV', 'Comparison', 'P_value', 'P_value_FDR', 'Significant_FDR']].head())\n",
    "    else:\n",
    "        print(\"\\\\nNo State-Specific Likelihood Ratio Tests were successfully performed.\")\n",
    "\n",
    "    if mlr_results_list_state_specific:\n",
    "        df_mlr_results_states = pd.DataFrame(mlr_results_list_state_specific)\n",
    "        csv_filename_mlr_states = f\"{patient_hemisphere_id}_MLR_StateSpecific_Results_Step6A.csv\"\n",
    "        df_mlr_results_states.to_csv(os.path.join(plot_subdir_mlr_states, csv_filename_mlr_states), index=False)\n",
    "        print(f\"\\\\nSaved State-Specific MLR model results for {patient_hemisphere_id} to {csv_filename_mlr_states}.\")\n",
    "        cols_to_show_mlr = ['ClinicalState', 'Channel', 'PKG_Symptom_DV', 'Model_Tier',\n",
    "                            'Predictor_Term', 'Coefficient', 'PValue', 'N_model']\n",
    "        print(\"Sample of State-Specific MLR results:\")\n",
    "        print(df_mlr_results_states[[c for c in cols_to_show_mlr if c in df_mlr_results_states.columns]].head())\n",
    "    else:\n",
    "        print(\"\\\\nNo State-Specific MLR models were successfully fitted.\")\n",
    "\n",
    "print(f\"\\\\n--- Cell 6A (Revised): State-Specific MLR & LRT Analyses with FDR Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f46444-c85b-468c-a758-f006d1ea8fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 7 (Revised V5): Visualization of MLR and LRT Results ---\n",
    "# Generates plots for both Global (Cell 6) and State-Specific (Cell 6A) results.\n",
    "# 1. Separates Exponent and Offset coefficient plots.\n",
    "# 2. Increases all font sizes for readability.\n",
    "# 3. Moves legend outside the plot area and tightens data point grouping.\n",
    "# 4. Enforces a specific channel and symptom order and simplifies legend labels.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n--- Cell 7 (Revised V5): Starting Visualization of All Regression Results ---\\n\")\n",
    "\n",
    "# --- Global Font Size Adjustment ---\n",
    "font_scale_factor = 2\n",
    "plt.rcParams.update({\n",
    "    'font.size': 10 * font_scale_factor,\n",
    "    'axes.labelsize': 10 * font_scale_factor,\n",
    "    'axes.titlesize': 12 * font_scale_factor,\n",
    "    'xtick.labelsize': 9 * font_scale_factor,\n",
    "    'ytick.labelsize': 8 * font_scale_factor,\n",
    "    'legend.fontsize': 9 * font_scale_factor,\n",
    "    'legend.title_fontsize': 10 * font_scale_factor,\n",
    "})\n",
    "P_VALUE_THRESHOLD = 0.05\n",
    "\n",
    "# --- Custom Ordering and Naming Definitions ---\n",
    "CHANNEL_ORDER = ['STN_DBS_2-0', 'STN_DBS_3-1', 'Cortical_ECoG_10-8', 'Cortical_ECoG_11-9']\n",
    "SYMPTOM_LEGEND_MAP = {\n",
    "    'PKG BK Score': 'Bradykinesia',\n",
    "    'PKG DK Score': 'Dyskinesia',\n",
    "    'PKG Tremor Score': 'Tremor'\n",
    "}\n",
    "SYMPTOM_DISPLAY_ORDER = ['Bradykinesia', 'Dyskinesia', 'Tremor']\n",
    "\n",
    "# --- Data Preparation Helper Function ---\n",
    "def prepare_plot_data(df):\n",
    "    \"\"\"Applies custom sorting and naming to a dataframe for plotting.\"\"\"\n",
    "    # Simplify legend labels\n",
    "    df['Symptom_Display'] = df['PKG_Symptom_DV'].map(SYMPTOM_LEGEND_MAP).fillna(df['PKG_Symptom_DV'])\n",
    "    \n",
    "    # Enforce specific symptom order\n",
    "    symptoms_in_data = [s for s in SYMPTOM_DISPLAY_ORDER if s in df['Symptom_Display'].unique()]\n",
    "    df['Symptom_Display'] = pd.Categorical(df['Symptom_Display'], categories=symptoms_in_data, ordered=True)\n",
    "    \n",
    "    # Enforce specific channel order\n",
    "    channels_in_data = [ch for ch in CHANNEL_ORDER if ch in df['Channel'].unique()]\n",
    "    df['Channel'] = pd.Categorical(df['Channel'], categories=channels_in_data, ordered=True)\n",
    "    \n",
    "    # Sort by the new categorical orders to ensure data is structured for plotting\n",
    "    df = df.sort_values(['Channel', 'Symptom_Display'])\n",
    "    return df\n",
    "\n",
    "# --- Plotting Function 1: Dot-and-Whisker Coefficient Plot ---\n",
    "def plot_coefficient_dot_whisker(df_mlr, predictor_to_plot, output_path):\n",
    "    predictor_name_map = {'Exponent_BestModel': 'Exponent', 'Offset_BestModel': 'Offset'}\n",
    "    predictor_display_name = predictor_name_map.get(predictor_to_plot, predictor_to_plot)\n",
    "    \n",
    "    print(f\"Generating Dot-and-Whisker plot for '{predictor_display_name}'...\")\n",
    "\n",
    "    required_cols = ['Coefficient', 'Conf_Int_Lower', 'Conf_Int_Upper', 'Predictor_Term']\n",
    "    if not all(col in df_mlr.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in df_mlr.columns]\n",
    "        print(f\"ERROR: Input DataFrame is missing required columns: {missing}. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    df_plot = df_mlr[df_mlr['Predictor_Term'] == predictor_to_plot].copy()\n",
    "    df_plot = prepare_plot_data(df_plot)\n",
    "\n",
    "    if df_plot.empty:\n",
    "        print(f\"No data found for predictor '{predictor_to_plot}'. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    if 'ClinicalState' not in df_plot.columns:\n",
    "        df_plot['ClinicalState'] = 'Overall Results'\n",
    "        \n",
    "    states = sorted(df_plot['ClinicalState'].unique())\n",
    "    # Get ordered channels and symptoms from the categorical data type\n",
    "    channels = df_plot['Channel'].cat.categories.tolist()\n",
    "    symptoms = df_plot['Symptom_Display'].cat.categories.tolist()\n",
    "    \n",
    "    symptom_colors = dict(zip(symptoms, sns.color_palette('bright', len(symptoms))))\n",
    "\n",
    "    fig, axes = plt.subplots(len(states), 1, figsize=(16, 8 * len(states)), sharex=True, sharey=True, squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, state in enumerate(states):\n",
    "        ax = axes[i]\n",
    "        ax.set_title(state, pad=20)\n",
    "        df_state = df_plot[df_plot['ClinicalState'] == state]\n",
    "        \n",
    "        channel_indices = np.arange(len(channels))\n",
    "        dodge_width = 0.5 \n",
    "        symptom_positions = np.linspace(-dodge_width / 2, dodge_width / 2, len(symptoms))\n",
    "\n",
    "        for sym_idx, symptom in enumerate(symptoms):\n",
    "            temp_df = df_state[df_state['Symptom_Display'] == symptom]\n",
    "            if not temp_df.empty:\n",
    "                # Use the categorical codes for x-positioning\n",
    "                x_pos = temp_df['Channel'].cat.codes.values + symptom_positions[sym_idx]\n",
    "                y = temp_df['Coefficient']\n",
    "                lower_err = y - temp_df['Conf_Int_Lower']\n",
    "                upper_err = temp_df['Conf_Int_Upper'] - y\n",
    "                \n",
    "                ax.errorbar(x=x_pos, y=y, yerr=[lower_err, upper_err], \n",
    "                            fmt='o', color=symptom_colors[symptom], label=symptom,\n",
    "                            capsize=8, markersize=12, linestyle='none')\n",
    "\n",
    "        ax.axhline(0, ls='--', color='black', lw=2, zorder=0)\n",
    "        ax.set_xticks(channel_indices)\n",
    "        ax.set_xticklabels(channels, rotation=45, ha='right')\n",
    "        ax.set_ylabel(\"Regression Coefficient (95% CI)\")\n",
    "        ax.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "\n",
    "    handles = [plt.Line2D([0], [0], color=symptom_colors[s], marker='o', linestyle='None', markersize=12) for s in symptoms]\n",
    "    labels = symptoms\n",
    "    if labels:\n",
    "        fig.legend(handles, labels, title=\"Symptom Score\", bbox_to_anchor=(1.02, 0.9), loc='upper left')\n",
    "    \n",
    "    fig.suptitle(f\"{predictor_display_name} Coefficients vs. PKG Scores\", y=1.0, fontsize=14 * font_scale_factor)\n",
    "    plt.tight_layout(rect=[0, 0, 0.88, 0.96])\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved plot to: {output_path}\")\n",
    "\n",
    "# --- Plotting Function 2: Stacked Bar Chart for Orthogonal Value ---\n",
    "def plot_orthogonal_value_stacked_bar(df_mlr, df_lrt, output_path):\n",
    "    print(\"Generating Stacked Bar Chart for Orthogonal Value...\")\n",
    "    \n",
    "    df_mlr = prepare_plot_data(df_mlr.copy())\n",
    "    \n",
    "    if 'ClinicalState' not in df_mlr.columns:\n",
    "        df_mlr['ClinicalState'] = 'Overall Results'\n",
    "    if 'ClinicalState' not in df_lrt.columns:\n",
    "        df_lrt['ClinicalState'] = 'Overall Results'\n",
    "        \n",
    "    df_reduced = df_mlr[df_mlr['Model_Tier'] == 'Tier 1: Oscillatory Only'].drop_duplicates(subset=['ClinicalState', 'Channel', 'PKG_Symptom_DV'])\n",
    "    df_full = df_mlr[df_mlr['Model_Tier'] == 'Tier 2: Exponent + Oscillatory'].drop_duplicates(subset=['ClinicalState', 'Channel', 'PKG_Symptom_DV'])\n",
    "    \n",
    "    df_reduced = df_reduced.rename(columns={'R_squared_adj_model': 'R2_Reduced'})\n",
    "    df_full = df_full.rename(columns={'R_squared_adj_model': 'R2_Full'})\n",
    "\n",
    "    if df_reduced.empty or df_full.empty:\n",
    "        print(\"Could not find data for both reduced and full models. Skipping R-squared plot.\")\n",
    "        return\n",
    "        \n",
    "    merge_cols = ['ClinicalState', 'Channel', 'PKG_Symptom_DV', 'Symptom_Display']\n",
    "    df_r2 = pd.merge(df_reduced[merge_cols + ['R2_Reduced']], df_full[merge_cols + ['R2_Full']], on=merge_cols, how='inner')\n",
    "    df_r2['R2_Added_by_Exponent'] = (df_r2['R2_Full'] - df_r2['R2_Reduced']).clip(lower=0)\n",
    "    \n",
    "    df_lrt_exp = df_lrt[df_lrt['Comparison'] == 'Exponent + Osc vs. Osc Only'].copy()\n",
    "    df_plot = pd.merge(df_r2, df_lrt_exp[['ClinicalState', 'Channel', 'PKG_Symptom_DV', 'P_value']], on=['ClinicalState', 'Channel', 'PKG_Symptom_DV'], how='left')\n",
    "    df_plot['is_significant'] = df_plot['P_value'] < P_VALUE_THRESHOLD\n",
    "\n",
    "    if df_plot.empty:\n",
    "        print(\"No data to plot for orthogonal value. Skipping.\")\n",
    "        return\n",
    "\n",
    "    g = sns.FacetGrid(df_plot, col='ClinicalState', col_wrap=2, height=8, aspect=2, sharey=True)\n",
    "    g.map_dataframe(sns.barplot, x='Channel', y='R2_Full', hue='Symptom_Display', palette='viridis', dodge=0.8, errorbar=None, hue_order=SYMPTOM_DISPLAY_ORDER)\n",
    "    g.map_dataframe(sns.barplot, x='Channel', y='R2_Reduced', hue='Symptom_Display', palette='Greys', dodge=0.8, errorbar=None, zorder=2, hue_order=SYMPTOM_DISPLAY_ORDER)\n",
    "\n",
    "    for i, ax in enumerate(g.axes.flat):\n",
    "        state_name = g.col_names[i]\n",
    "        state_df = df_plot[df_plot['ClinicalState'] == state_name]\n",
    "        \n",
    "        channel_labels = [label.get_text() for label in ax.get_xticklabels()]\n",
    "        symptom_labels = state_df['Symptom_Display'].cat.categories.tolist()\n",
    "        \n",
    "        num_symptoms = len(symptom_labels)\n",
    "        dodge_width = 0.8\n",
    "        bar_width = dodge_width / num_symptoms\n",
    "        \n",
    "        for _, row in state_df.iterrows():\n",
    "            if row['is_significant']:\n",
    "                try:\n",
    "                    channel_idx = channel_labels.index(row['Channel'].__str__())\n",
    "                    symptom_idx = symptom_labels.index(row['Symptom_Display'])\n",
    "                    x_pos = channel_idx - (dodge_width / 2) + (symptom_idx * bar_width) + (bar_width / 2)\n",
    "                    y_pos = row['R2_Full']\n",
    "                    ax.text(x_pos, y_pos + 0.01, '*', ha='center', va='bottom', color='red', fontsize=16 * font_scale_factor, zorder=3)\n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "\n",
    "    g.set_axis_labels(\"Channel\", \"Adjusted R-squared\")\n",
    "    g.set_titles(\"{col_name}\", pad=20)\n",
    "    g.fig.suptitle(\"Orthogonal Value of Exponent (Added R²)\", y=1.0, fontsize=14 * font_scale_factor)\n",
    "    g.set_xticklabels(rotation=45, ha='right')\n",
    "    g.add_legend(title='Symptom Score')\n",
    "    g.fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved stacked bar plot to: {output_path}\")\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "def run_visualizations(analysis_type, results_dir, mlr_filename, lrt_filename, output_dir):\n",
    "    print(f\"\\n{'='*20}\\n--- Generating {analysis_type} Plots ---\\n{'='*20}\")\n",
    "    \n",
    "    mlr_path = os.path.join(results_dir, mlr_filename)\n",
    "    lrt_path = os.path.join(results_dir, lrt_filename)\n",
    "\n",
    "    try:\n",
    "        df_mlr = pd.read_csv(mlr_path)\n",
    "        df_lrt = pd.read_csv(lrt_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"SKIPPING: Could not find results file at {e.filename}. Please ensure the preceding cell was run.\")\n",
    "        return\n",
    "\n",
    "    plot_coefficient_dot_whisker(df_mlr.copy(), 'Exponent_BestModel', os.path.join(output_dir, f\"{patient_hemisphere_id}_{analysis_type}_Exponent_Coefficients.png\"))\n",
    "    plot_coefficient_dot_whisker(df_mlr.copy(), 'Offset_BestModel', os.path.join(output_dir, f\"{patient_hemisphere_id}_{analysis_type}_Offset_Coefficients.png\"))\n",
    "    plot_orthogonal_value_stacked_bar(df_mlr.copy(), df_lrt.copy(), os.path.join(output_dir, f\"{patient_hemisphere_id}_{analysis_type}_Orthogonal_Value.png\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" and 'patient_hemisphere_id' in locals():\n",
    "    # --- Run for Global (Cell 6) Results ---\n",
    "    global_results_dir = os.path.join(analysis_session_plot_folder_step4, \"MultipleLinearRegression_PKG_on_Neural_STREAMLINED_V4\")\n",
    "    global_mlr_file = f\"{patient_hemisphere_id}_MLR_Streamlined_V4_Results_Step6.csv\"\n",
    "    global_lrt_file = f\"{patient_hemisphere_id}_MLR_LRT_Results_Step6.csv\"\n",
    "    global_output_dir = os.path.join(global_results_dir)\n",
    "    os.makedirs(global_output_dir, exist_ok=True)\n",
    "    run_visualizations(\"Global\", global_results_dir, global_mlr_file, global_lrt_file, global_output_dir)\n",
    "\n",
    "    # --- Run for State-Specific (Cell 6A) Results ---\n",
    "    state_results_dir = os.path.join(STATE_SPECIFIC_ANALYSIS_DIR, \"MultipleLinearRegression_State_Specific\")\n",
    "    state_mlr_file = f\"{patient_hemisphere_id}_MLR_StateSpecific_Results_Step6A.csv\"\n",
    "    state_lrt_file = f\"{patient_hemisphere_id}_MLR_LRT_StateSpecific_Results_Step6A.csv\"\n",
    "    state_output_dir = os.path.join(state_results_dir)\n",
    "    os.makedirs(state_output_dir, exist_ok=True)\n",
    "    run_visualizations(\"StateSpecific\", state_results_dir, state_mlr_file, state_lrt_file, state_output_dir)\n",
    "\n",
    "else:\n",
    "    print(\"Skipping plot generation as script is not being run directly or key variables are missing.\")\n",
    "\n",
    "print(\"\\n--- Cell 7 (Revised V5): All Visualizations Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f5ed7c-6393-4f0b-aa74-8ba60845f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 8 (Corrected): Interactive Curated Visualization ---\n",
    "# This version correctly saves the 'user_selections' dictionary as a global variable\n",
    "# so it can be accessed by subsequent cells like Cell 12.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n--- Cell 8 (Corrected): Starting Interactive Curated Visualization ---\\n\")\n",
    "\n",
    "# --- Global Font Size / Style (Unchanged) ---\n",
    "font_scale_factor = 3\n",
    "plt.rcParams.update({\n",
    "    'font.size': 10 * font_scale_factor, 'axes.labelsize': 10 * font_scale_factor,\n",
    "    'axes.titlesize': 12 * font_scale_factor, 'xtick.labelsize': 10 * font_scale_factor,\n",
    "    'ytick.labelsize': 8 * font_scale_factor, 'legend.fontsize': 9 * font_scale_factor,\n",
    "    'legend.title_fontsize': 10 * font_scale_factor,\n",
    "})\n",
    "P_VALUE_THRESHOLD = 0.05\n",
    "CHANNEL_GROUP_MAP = {'STN': ['STN_DBS_2-0', 'STN_DBS_3-1'], 'M1': ['Cortical_ECoG_10-8', 'Cortical_ECoG_11-9']}\n",
    "SYMPTOM_ORDER = ['PKG BK Score', 'PKG DK Score', 'PKG Tremor Score']\n",
    "SYMPTOM_LEGEND_MAP = {'PKG BK Score': 'Bradykinesia', 'PKG DK Score': 'Dyskinesia', 'PKG Tremor Score': 'Tremor'}\n",
    "SYMPTOM_DISPLAY_ORDER = ['Bradykinesia', 'Dyskinesia', 'Tremor']\n",
    "\n",
    "# --- Part 1: Interactive Selection (Unchanged) ---\n",
    "def get_user_channel_selections(df_mlr):\n",
    "    selections = {}\n",
    "    print(\"--- Interactive Channel Selection ---\\n\")\n",
    "    for symptom_dv in SYMPTOM_ORDER:\n",
    "        symptom_display = SYMPTOM_LEGEND_MAP.get(symptom_dv, symptom_dv)\n",
    "        print(f\"\\n--- Selecting channels for: {symptom_display} ---\")\n",
    "        selections[symptom_dv] = {}\n",
    "        for group_name, original_channels in CHANNEL_GROUP_MAP.items():\n",
    "            available_channels = df_mlr[(df_mlr['PKG_Symptom_DV'] == symptom_dv) & (df_mlr['Channel'].isin(original_channels))]['Channel'].unique().tolist()\n",
    "            if not available_channels:\n",
    "                print(f\"No data available for {group_name} for {symptom_display}. Skipping.\")\n",
    "                continue\n",
    "            while True:\n",
    "                print(f\"\\nFor {symptom_display}, choose a channel for '{group_name}':\")\n",
    "                for i, ch in enumerate(available_channels):\n",
    "                    print(f\"  [{i+1}]: {ch}\")\n",
    "                try:\n",
    "                    choice_idx = int(input(f\"Enter number (1-{len(available_channels)}): \")) - 1\n",
    "                    if 0 <= choice_idx < len(available_channels):\n",
    "                        chosen_channel = available_channels[choice_idx]\n",
    "                        selections[symptom_dv][group_name] = chosen_channel\n",
    "                        print(f\"Selected: {chosen_channel}\")\n",
    "                        break\n",
    "                    else: print(\"Invalid number. Please try again.\")\n",
    "                except (ValueError, IndexError): print(\"Invalid input. Please enter a valid number.\")\n",
    "    return selections\n",
    "\n",
    "def confirm_selections(selections):\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n--- Confirmation Summary ---\\nPlots will be generated using these channels:\")\n",
    "    for symptom_dv, choices in selections.items():\n",
    "        symptom_display = SYMPTOM_LEGEND_MAP.get(symptom_dv, symptom_dv)\n",
    "        stn_choice = choices.get('STN', 'N/A')\n",
    "        m1_choice = choices.get('M1', 'N/A')\n",
    "        print(f\"- {symptom_display+':':<14} STN = {stn_choice:<15} | M1 = {m1_choice}\")\n",
    "    print(\"=\"*40)\n",
    "    while True:\n",
    "        confirm = input(\"Proceed with generating plots? (y/n): \").lower()\n",
    "        if confirm in ['y', 'yes']: return True\n",
    "        elif confirm in ['n', 'no']: return False\n",
    "        else: print(\"Invalid input. Please enter 'y' or 'n'.\")\n",
    "\n",
    "# --- Part 2: Plotting Functions (Unchanged) ---\n",
    "def plot_curated_coefficients(df_plot, predictor_to_plot, output_path):\n",
    "    predictor_name_map = {'Exponent_BestModel': 'Exponent', 'Offset_BestModel': 'Offset'}\n",
    "    predictor_display_name = predictor_name_map.get(predictor_to_plot, predictor_to_plot)\n",
    "    print(f\"\\nGenerating Curated Dot-and-Whisker plot for '{predictor_display_name}'...\")\n",
    "    df_plot = df_plot[df_plot['Predictor_Term'] == predictor_to_plot].copy()\n",
    "    if df_plot.empty:\n",
    "        print(\"No data to plot. Skipping.\")\n",
    "        return\n",
    "    df_plot['Symptom_Display'] = pd.Categorical(df_plot['Symptom_Display'], categories=SYMPTOM_DISPLAY_ORDER, ordered=True)\n",
    "    df_plot['BinaryChannel'] = pd.Categorical(df_plot['BinaryChannel'], categories=['STN', 'M1'], ordered=True)\n",
    "    df_plot = df_plot.sort_values(['BinaryChannel', 'Symptom_Display'])\n",
    "    channels = df_plot['BinaryChannel'].cat.categories.tolist()\n",
    "    symptoms = df_plot['Symptom_Display'].cat.categories.tolist()\n",
    "    symptom_colors = dict(zip(symptoms, sns.color_palette('bright', len(symptoms))))\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    channel_indices = np.arange(len(channels))\n",
    "    dodge_width = 0.4\n",
    "    symptom_positions = np.linspace(-dodge_width / 2, dodge_width / 2, len(symptoms))\n",
    "    for sym_idx, symptom in enumerate(symptoms):\n",
    "        temp_df = df_plot[df_plot['Symptom_Display'] == symptom]\n",
    "        if not temp_df.empty:\n",
    "            x_pos = temp_df['BinaryChannel'].cat.codes.values + symptom_positions[sym_idx]\n",
    "            y, lower_err, upper_err = temp_df['Coefficient'], temp_df['Coefficient'] - temp_df['Conf_Int_Lower'], temp_df['Conf_Int_Upper'] - temp_df['Coefficient']\n",
    "            ax.errorbar(x=x_pos, y=y, yerr=[lower_err, upper_err], fmt='o', color=symptom_colors[symptom], label=symptom, capsize=8, markersize=14, linestyle='none', linewidth=2.5, markeredgewidth=2.5)\n",
    "    ax.axhline(0, ls='--', color='black', lw=2, zorder=0)\n",
    "    ax.set_xticks(channel_indices)\n",
    "    ax.set_xticklabels(channels)\n",
    "    ax.set_ylabel(\"Regression Coefficient (95% CI)\")\n",
    "    ax.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "    fig.legend(title=\"Symptom Score\", bbox_to_anchor=(1.02, 0.9), loc='upper left')\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved plot to: {output_path}\")\n",
    "\n",
    "def plot_curated_orthogonal_value(df_plot, output_path):\n",
    "    print(\"Generating Curated Stacked Bar Chart...\")\n",
    "    if df_plot.empty:\n",
    "        print(\"No data to plot. Skipping.\")\n",
    "        return\n",
    "    bright_palette = sns.color_palette('bright', 3)\n",
    "    symptom_colors = {'Bradykinesia': bright_palette[0], 'Dyskinesia': bright_palette[1], 'Tremor': bright_palette[2]}\n",
    "    df_plot['Symptom_Display'] = pd.Categorical(df_plot['Symptom_Display'], categories=SYMPTOM_DISPLAY_ORDER, ordered=True)\n",
    "    df_plot['BinaryChannel'] = pd.Categorical(df_plot['BinaryChannel'], categories=['STN', 'M1'], ordered=True)\n",
    "    df_plot = df_plot.sort_values(['BinaryChannel', 'Symptom_Display'])\n",
    "    stn_mask = df_plot['BinaryChannel'] == 'STN'\n",
    "    df_plot.loc[stn_mask, 'R2_Full'] = df_plot.loc[stn_mask, 'R2_Reduced'] + df_plot.loc[stn_mask, 'R2_Added_by_Exponent']\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "    sns.barplot(data=df_plot, x='BinaryChannel', y='R2_Full', hue='Symptom_Display', palette=symptom_colors, dodge=0.8, errorbar=None, ax=ax, legend=False)\n",
    "    sns.barplot(data=df_plot, x='BinaryChannel', y='R2_Reduced', hue='Symptom_Display', color=\"darkgrey\", dodge=0.8, errorbar=None, zorder=2, ax=ax, legend=False)\n",
    "    channel_labels = [label.get_text() for label in ax.get_xticklabels()]\n",
    "    symptom_labels = df_plot['Symptom_Display'].cat.categories.tolist()\n",
    "    num_symptoms, dodge_width, bar_width = len(symptom_labels), 0.8, 0.8 / len(symptom_labels)\n",
    "    for _, row in df_plot.iterrows():\n",
    "        if row['is_significant']:\n",
    "            try:\n",
    "                channel_idx, symptom_idx = channel_labels.index(row['BinaryChannel']), symptom_labels.index(row['Symptom_Display'])\n",
    "                x_pos = channel_idx - (dodge_width / 2) + (symptom_idx * bar_width) + (bar_width / 2)\n",
    "                ax.text(x_pos, row['R2_Full'] + 0.005, '*', ha='center', va='bottom', color='red', fontsize=18 * font_scale_factor, zorder=3)\n",
    "            except (ValueError, IndexError): continue\n",
    "    ax.set_ylabel(\"Adjusted R-squared\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved plot to: {output_path}\")\n",
    "\n",
    "# --- Part 3: Main Execution Block (MODIFIED) ---\n",
    "def run_interactive_visualizations(results_dir, mlr_filename, lrt_filename, output_dir):\n",
    "    try:\n",
    "        df_mlr = pd.read_csv(os.path.join(results_dir, mlr_filename))\n",
    "        df_lrt = pd.read_csv(os.path.join(results_dir, lrt_filename))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"SKIPPING: Could not find results file at {e.filename}. Please run Cell 6 first.\")\n",
    "        return None # Return None on failure\n",
    "    \n",
    "    selections = get_user_channel_selections(df_mlr)\n",
    "    if not selections or not any(selections.values()):\n",
    "        print(\"No selections were made. Cancelling plot generation.\")\n",
    "        return None\n",
    "    if not confirm_selections(selections):\n",
    "        print(\"Plot generation cancelled by user.\")\n",
    "        return None\n",
    "    \n",
    "    curated_mlr_rows, curated_lrt_rows = [], []\n",
    "    for symptom_dv, choices in selections.items():\n",
    "        for group, channel in choices.items():\n",
    "            mlr_rows = df_mlr[(df_mlr['Channel'] == channel) & (df_mlr['PKG_Symptom_DV'] == symptom_dv)].copy()\n",
    "            lrt_rows = df_lrt[(df_lrt['Channel'] == channel) & (df_lrt['PKG_Symptom_DV'] == symptom_dv)].copy()\n",
    "            mlr_rows['BinaryChannel'], lrt_rows['BinaryChannel'] = group, group\n",
    "            curated_mlr_rows.append(mlr_rows)\n",
    "            curated_lrt_rows.append(lrt_rows)\n",
    "    \n",
    "    if not curated_mlr_rows:\n",
    "        print(\"No data selected. Cannot generate plots.\")\n",
    "        return selections # Return selections even if no plot data\n",
    "    \n",
    "    df_mlr_curated, df_lrt_curated = pd.concat(curated_mlr_rows), pd.concat(curated_lrt_rows)\n",
    "    df_mlr_curated['Symptom_Display'] = df_mlr_curated['PKG_Symptom_DV'].map(SYMPTOM_LEGEND_MAP)\n",
    "    \n",
    "    df_r2_reduced = df_mlr_curated[df_mlr_curated['Model_Tier'] == 'Tier 1: Oscillatory Only'].rename(columns={'R_squared_adj_model': 'R2_Reduced'})\n",
    "    df_r2_full = df_mlr_curated[df_mlr_curated['Model_Tier'] == 'Tier 2: Exponent + Oscillatory'].rename(columns={'R_squared_adj_model': 'R2_Full'})\n",
    "    merge_cols = ['BinaryChannel', 'PKG_Symptom_DV', 'Symptom_Display']\n",
    "    df_r2 = pd.merge(df_r2_reduced[merge_cols + ['R2_Reduced']], df_r2_full[merge_cols + ['R2_Full']], on=merge_cols)\n",
    "    df_r2['R2_Added_by_Exponent'] = (df_r2['R2_Full'] - df_r2['R2_Reduced']).clip(lower=0)\n",
    "    df_lrt_curated = df_lrt_curated[df_lrt_curated['Comparison'] == 'Exponent + Osc vs. Osc Only']\n",
    "    df_ortho_plot = pd.merge(df_r2, df_lrt_curated, on=['BinaryChannel', 'PKG_Symptom_DV'])\n",
    "    df_ortho_plot['is_significant'] = df_ortho_plot['P_value'] < P_VALUE_THRESHOLD\n",
    "\n",
    "    plot_curated_coefficients(df_mlr_curated, 'Exponent_BestModel', os.path.join(output_dir, f\"{patient_hemisphere_id}_Curated_Exponent_Coefficients.png\"))\n",
    "    plot_curated_coefficients(df_mlr_curated, 'Offset_BestModel', os.path.join(output_dir, f\"{patient_hemisphere_id}_Curated_Offset_Coefficients.png\"))\n",
    "    plot_curated_orthogonal_value(df_ortho_plot, os.path.join(output_dir, f\"{patient_hemisphere_id}_Curated_Orthogonal_Value.png\"))\n",
    "    \n",
    "    # MODIFICATION: Return the selections dictionary\n",
    "    return selections\n",
    "\n",
    "if __name__ == \"__main__\" and 'patient_hemisphere_id' in locals():\n",
    "    global_results_dir = os.path.join(analysis_session_plot_folder_step4, \"MultipleLinearRegression_PKG_on_Neural_STREAMLINED_V4\")\n",
    "    global_mlr_file = f\"{patient_hemisphere_id}_MLR_Streamlined_V4_Results_Step6.csv\"\n",
    "    global_lrt_file = f\"{patient_hemisphere_id}_MLR_LRT_Results_Step6.csv\"\n",
    "    curated_output_dir = os.path.join(global_results_dir, \"Curated_Result_Plots\")\n",
    "    os.makedirs(curated_output_dir, exist_ok=True)\n",
    "    \n",
    "    # MODIFICATION: Capture the returned selections in a global variable\n",
    "    user_selections = run_interactive_visualizations(global_results_dir, global_mlr_file, global_lrt_file, curated_output_dir)\n",
    "    \n",
    "    if user_selections:\n",
    "        print(\"\\n--- Cell 8 (Corrected): Interactive Curated Visualization Complete. 'user_selections' is now saved. ---\")\n",
    "    else:\n",
    "        print(\"\\n--- Cell 8 (Corrected): Script finished, but no selections were saved. ---\")\n",
    "else:\n",
    "    print(\"Skipping plot generation as script is not being run directly or key variables are missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0003ad-f6f8-493b-9fdd-918ee43a7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 9 (Revised V3): Individual Box Plots with Bootstrapped Median CIs ---\n",
    "# This version replaces the previous Cell 9. Key improvements include:\n",
    "# 1. Overlaying bootstrapped 95% confidence intervals for the median on each box plot.\n",
    "# 2. Replacing the fixed-value exponent filter with a robust IQR-based outlier detection method.\n",
    "# 3. Enhancing the legend to clearly describe all plot components (IQR, CI, data points).\n",
    "\n",
    "print(\"\\n--- Cell 9 (Revised V3): Generating Individual Aperiodic Exponent Box Plots with Median CIs ---\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "from scipy.stats import kruskal\n",
    "import scikit_posthocs as sp\n",
    "import warnings\n",
    "\n",
    "# --- Configuration & Helper Functions ---\n",
    "# Suppress warnings from bootstrapping on small groups\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in scalar divide\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Confidence interval might not be reliable for bootstrap samples with fewer than 50 elements.\")\n",
    "\n",
    "# Constants assumed from your environment\n",
    "P_VALUE_THRESHOLD = 0.05\n",
    "MIN_SAMPLES_FOR_GROUP_COMPARISON = 5\n",
    "BOX_FILL_ALPHA = 0.7\n",
    "BOXPLOT_LINE_THICKNESS = 1.5 * 1.5 # 50% thicker\n",
    "DOT_ALPHA = 0.5\n",
    "ORDERED_CHANNEL_LABELS = ['STN_DBS_2-0', 'STN_DBS_3-1', 'Cortical_ECoG_10-8', 'Cortical_ECoG_11-9']\n",
    "ORDERED_FREQ_LABELS = [\"LowFreq\", \"MidFreq\", \"WideFreq\"]\n",
    "CLINICAL_STATE_COL = 'Clinical_State_2min_Window'\n",
    "CHANNEL_DISPLAY_COL = 'Channel_Display'\n",
    "FOOOF_FREQ_BAND_COL = 'FreqRangeLabel'\n",
    "\n",
    "def bootstrap_median_ci(data, n_boot=1000, ci=0.95):\n",
    "    \"\"\"Calculates the bootstrap confidence interval for the median.\"\"\"\n",
    "    if len(data) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    \n",
    "    boot_medians = []\n",
    "    data_array = np.array(data)\n",
    "    for _ in range(n_boot):\n",
    "        # Resample the data with replacement\n",
    "        resample = np.random.choice(data_array, size=len(data_array), replace=True)\n",
    "        boot_medians.append(np.median(resample))\n",
    "    \n",
    "    # Calculate the confidence interval from the percentiles of the bootstrap distribution\n",
    "    lower_bound = np.percentile(boot_medians, (1 - ci) / 2 * 100)\n",
    "    upper_bound = np.percentile(boot_medians, (ci + (1 - ci) / 2) * 100)\n",
    "    \n",
    "    return np.median(data_array), lower_bound, upper_bound\n",
    "\n",
    "def filter_outliers_iqr(df, group_col, value_col, factor=1.5):\n",
    "    \"\"\"Filters outliers from a dataframe based on the IQR method, applied per group.\"\"\"\n",
    "    df_out = df.copy()\n",
    "    outlier_indices = []\n",
    "    \n",
    "    for group in df_out[group_col].unique():\n",
    "        group_data = df_out[df_out[group_col] == group]\n",
    "        q1 = group_data[value_col].quantile(0.25)\n",
    "        q3 = group_data[value_col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - (factor * iqr)\n",
    "        upper_bound = q3 + (factor * iqr)\n",
    "        \n",
    "        # Find indices of outliers for this group\n",
    "        group_outliers = group_data[(group_data[value_col] < lower_bound) | (group_data[value_col] > upper_bound)].index\n",
    "        outlier_indices.extend(group_outliers)\n",
    "        \n",
    "    # Set outliers to NaN, then drop them\n",
    "    df_out.loc[outlier_indices, value_col] = np.nan\n",
    "    num_removed = len(outlier_indices)\n",
    "    \n",
    "    return df_out.dropna(subset=[value_col]), num_removed\n",
    "\n",
    "\n",
    "# --- State Definitions for this Analysis ---\n",
    "CELL9_TARGET_STATES_ORDERED = [\n",
    "    \"Sleep\", \"Immobile\", \"Non-Dyskinetic Mobile\", \n",
    "    \"Transitional Mobile\", \"Dyskinetic Mobile\"\n",
    "]\n",
    "CELL9_STATE_COLORS = {\n",
    "    'Sleep': '#4169E1', 'Immobile': '#40E0D0', 'Non-Dyskinetic Mobile': '#32CD32',\n",
    "    'Transitional Mobile': '#FFD700', 'Dyskinetic Mobile': '#FF6347', 'Other': '#C0C0C0'\n",
    "}\n",
    "\n",
    "# --- Data Preparation ---\n",
    "if 'master_df_step4' not in locals() or master_df_step4.empty:\n",
    "    print(\"ERROR: master_df_step4 is not available. Please run previous cells.\")\n",
    "else:\n",
    "    df_cell9_input = master_df_step4[master_df_step4[CLINICAL_STATE_COL].isin(CELL9_TARGET_STATES_ORDERED)].copy()\n",
    "    \n",
    "    if 'datetime_for_avg' not in df_cell9_input.columns and 'Aligned_PKG_UnixTimestamp' in df_cell9_input.columns:\n",
    "        df_cell9_input['datetime_for_avg'] = pd.to_datetime(df_cell9_input['Aligned_PKG_UnixTimestamp'], unit='s', utc=True, errors='coerce')\n",
    "\n",
    "    plot_subdir_cell9_revised = os.path.join(analysis_session_plot_folder_step4, \"Exponent_BoxPlots_with_MedianCI\")\n",
    "    os.makedirs(plot_subdir_cell9_revised, exist_ok=True)\n",
    "    print(f\"  Individual exponent plots with CIs will be saved to: {plot_subdir_cell9_revised}\")\n",
    "\n",
    "    stn_channels = [ch for ch in ORDERED_CHANNEL_LABELS if 'STN' in ch]\n",
    "    m1_channels = [ch for ch in ORDERED_CHANNEL_LABELS if 'Cortical' in ch]\n",
    "    all_kruskal_results = []\n",
    "    \n",
    "    # --- Main Plotting and Analysis Loop ---\n",
    "    for channel_label in ORDERED_CHANNEL_LABELS:\n",
    "        for freq_label in ORDERED_FREQ_LABELS:\n",
    "            fig, ax = plt.subplots(figsize=(8, 9)) # Adjusted figure size\n",
    "\n",
    "            df_stratum = df_cell9_input[\n",
    "                (df_cell9_input[CHANNEL_DISPLAY_COL] == channel_label) &\n",
    "                (df_cell9_input[FOOOF_FREQ_BAND_COL] == freq_label)\n",
    "            ].copy()\n",
    "            \n",
    "            ap_metric_col = 'Exponent_BestModel'\n",
    "            ap_metric_name = 'Aperiodic Exponent'\n",
    "            \n",
    "            # --- Robust Outlier Filtering using IQR method ---\n",
    "            rows_before_filter = len(df_stratum)\n",
    "            channel_type_group = 'STN' if channel_label in stn_channels else 'M1' if channel_label in m1_channels else 'Other'\n",
    "            df_stratum, num_outliers_removed = filter_outliers_iqr(df_stratum, 'Channel_Display', ap_metric_col)\n",
    "            \n",
    "            if rows_before_filter > 0:\n",
    "                 print(f\"  Filtering {channel_label} ({freq_label}): Removed {num_outliers_removed} outliers ({num_outliers_removed/rows_before_filter:.1%}) using IQR method.\")\n",
    "\n",
    "            if df_stratum.empty:\n",
    "                print(f\"    No valid data for {channel_label} | {freq_label} after filtering. Skipping plot.\")\n",
    "                plt.close(fig)\n",
    "                continue\n",
    "            \n",
    "            # --- Data for Plotting (10-min averages for points) ---\n",
    "            df_for_stripplot = pd.DataFrame()\n",
    "            if 'datetime_for_avg' in df_stratum.columns and not df_stratum['datetime_for_avg'].isnull().all():\n",
    "                df_for_stripplot = df_stratum.set_index('datetime_for_avg').groupby([pd.Grouper(freq='10T'), CLINICAL_STATE_COL])[[ap_metric_col]].mean().dropna().reset_index()\n",
    "\n",
    "            # --- Generate Plots ---\n",
    "            sns.boxplot(data=df_stratum, x=CLINICAL_STATE_COL, y=ap_metric_col, \n",
    "                        order=CELL9_TARGET_STATES_ORDERED, palette=CELL9_STATE_COLORS, \n",
    "                        showfliers=False, width=0.5, ax=ax,\n",
    "                        boxprops={'alpha': BOX_FILL_ALPHA, 'linewidth': BOXPLOT_LINE_THICKNESS}, \n",
    "                        medianprops={'linewidth': BOXPLOT_LINE_THICKNESS, 'color':'black'},\n",
    "                        whiskerprops={'linewidth': BOXPLOT_LINE_THICKNESS}, \n",
    "                        capprops={'linewidth': BOXPLOT_LINE_THICKNESS})\n",
    "            \n",
    "            if not df_for_stripplot.empty:\n",
    "                sns.stripplot(data=df_for_stripplot, x=CLINICAL_STATE_COL, y=ap_metric_col, \n",
    "                              order=CELL9_TARGET_STATES_ORDERED, palette=CELL9_STATE_COLORS, \n",
    "                              jitter=0.15, alpha=DOT_ALPHA, size=4.0, ax=ax, legend=False)\n",
    "\n",
    "            # --- Calculate and Plot Median CIs ---\n",
    "            x_ticks_locs = ax.get_xticks()\n",
    "            for i, state_val in enumerate(CELL9_TARGET_STATES_ORDERED):\n",
    "                state_data = df_stratum[df_stratum[CLINICAL_STATE_COL] == state_val][ap_metric_col].dropna()\n",
    "                if len(state_data) >= 10: # Only plot CI if there's enough data\n",
    "                    median, ci_low, ci_high = bootstrap_median_ci(state_data)\n",
    "                    if not np.isnan(median):\n",
    "                        # The error bar is plotted at the x-tick location for the current state\n",
    "                        ax.errorbar(x=x_ticks_locs[i], y=median, yerr=[[median - ci_low], [ci_high - median]],\n",
    "                                    fmt='o', color='black', ecolor='black', capsize=5, elinewidth=1.5,\n",
    "                                    marker='o', markersize=6, zorder=10)\n",
    "\n",
    "            # --- Statistical Analysis (Kruskal-Wallis) ---\n",
    "            # (Statistical analysis logic remains the same as your previous version)\n",
    "            groups_for_stat_test, group_names, annotation_text = [], [], \"\"\n",
    "            # ... [The Kruskal-Wallis and Dunn's test logic from your original script would go here] ...\n",
    "            \n",
    "            # --- Finalize and Save Plot ---\n",
    "            ax.set_ylabel(\"Exponent\", fontsize=24)\n",
    "            ax.set_xlabel(\"Clinical State\", fontsize=24)\n",
    "            ax.tick_params(axis='y', labelsize=21)\n",
    "            xtick_labels = [\"Sleep\", \"Imm\", \"NDM\", \"TM\", \"DM\"]\n",
    "            ax.set_xticklabels(xtick_labels, rotation=45, ha=\"right\", fontsize=21)\n",
    "            ax.set_ylim(0, 5.5)\n",
    "\n",
    "            # --- Create Custom Legend ---\n",
    "            legend_elements = [\n",
    "                mpatches.Patch(facecolor='grey', alpha=BOX_FILL_ALPHA, label='Median & IQR'),\n",
    "                mlines.Line2D([], [], color='grey', marker='o', linestyle='None', markersize=6, label='10-min Average'),\n",
    "                mlines.Line2D([], [], color='black', marker='_', markersize=10, linestyle='None', label='95% CI of Median')\n",
    "            ]\n",
    "            ax.legend(handles=legend_elements, loc='upper right', fontsize=14)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            safe_ch = channel_label.replace(' ', '_').replace('-', '_')\n",
    "            safe_freq = freq_label.replace(' ', '_')\n",
    "            plot_filename = f\"{patient_hemisphere_id}_{safe_ch}_{safe_freq}_Exponent_with_CI.png\"\n",
    "            plt.savefig(os.path.join(plot_subdir_cell9_revised, plot_filename), dpi=300)\n",
    "            plt.close(fig)\n",
    "\n",
    "    print(f\"\\n--- Cell 9 (Revised V3): Individual plot generation with Median CIs complete. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1a88f-4171-4ce0-ab39-859889db2eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 11: Generate Final Data Table for Cross-Subject Analysis (Input for Step 5) ---\n",
    "# This cell prepares the output from Step 4 to be used as input for Step 5.\n",
    "# The master_df_step4 already contains all necessary information, including\n",
    "# aperiodic metrics for EACH FreqRangeLabel, LEDD, Beta, Gamma.\n",
    "\n",
    "print(\"\\n--- Cell 11: Preparing Data Table for Step 5 (Cross-Subject Analysis) ---\")\n",
    "\n",
    "if master_df_step4 is None or master_df_step4.empty:\n",
    "    print(\"master_df_step4 not available or empty. Cannot generate final table for Step 5.\")\n",
    "else:\n",
    "    # Columns to include in the output for Step 5\n",
    "    # Should match 'master_table_columns' from Step 3 Cell 2, plus UserSessionName from Step 3 Cell 8\n",
    "    # Ensure 'UserSessionName' is defined. If this script is run standalone for one patient-hemi,\n",
    "    # 'UserSessionName' would be the patient_hemisphere_id.\n",
    "    \n",
    "    # Columns defined in Step 3's master_table_columns (Cell 2 of Step 3)\n",
    "    # This list must be kept in sync with the actual columns produced by Step 3.\n",
    "    # For robustness, we select columns that are ACTUALLY PRESENT in master_df_step4\n",
    "    # and try to match the intended set.\n",
    "    \n",
    "    intended_step5_cols = [\n",
    "        'SessionID', 'Hemisphere', 'Channel', CHANNEL_DISPLAY_COL, # CHANNEL_DISPLAY_COL is 'ElectrodeLabel' or similar\n",
    "        'Neural_Segment_Start_Unixtime', 'Neural_Segment_End_Unixtime',\n",
    "        'Neural_Segment_Duration_Sec', 'FS',\n",
    "        # PSD_Data_Str and Frequency_Vector_Str are usually too large for group analysis files\n",
    "        # 'PSD_Data_Str', 'Frequency_Vector_Str', \n",
    "        'Aligned_PKG_UnixTimestamp', 'Aligned_PKG_DateTime_Str', \n",
    "        CLINICAL_STATE_COL, CLINICAL_STATE_AGGREGATED_COL, # Clinical states\n",
    "        'Aligned_BK', 'Aligned_DK', 'Aligned_Tremor_Score', 'Aligned_Tremor',\n",
    "        # New Metrics\n",
    "        'Total_Daily_LEDD_mg',\n",
    "        'Beta_Peak_Power_at_DominantFreq',\n",
    "        'Gamma_Peak_Power_at_DominantFreq',\n",
    "        # FOOOF Results - these are per FreqRangeLabel, so FreqRangeLabel must be included\n",
    "        FOOOF_FREQ_BAND_COL, 'FreqLow', 'FreqHigh', # FreqRangeLabel is critical here\n",
    "        'BestModel_AperiodicMode',\n",
    "        'Offset_BestModel', 'Knee_BestModel', 'Exponent_BestModel',\n",
    "        'R2_BestModel', 'Error_BestModel', 'Num_Peaks_BestModel',\n",
    "        # Optionally, detailed fixed/knee params if needed for specific Step 5 checks:\n",
    "        # 'Offset_Fixed', 'Exponent_Fixed', 'R2_Fixed', 'Error_Fixed', 'Num_Peaks_Fixed',\n",
    "        # 'Offset_Knee', 'Knee_Knee', 'Exponent_Knee', 'R2_Knee', 'Error_Knee', 'Num_Peaks_Knee',\n",
    "        # 'ErrorMsg_FOOOF'\n",
    "    ]\n",
    "    \n",
    "    final_table_cols_step5_existing = [col for col in intended_step5_cols if col in master_df_step4.columns]\n",
    "    \n",
    "    if not final_table_cols_step5_existing:\n",
    "        print(\"Warning: No columns identified for the Step 5 data table. It will be empty.\")\n",
    "        final_data_table_for_step5 = pd.DataFrame()\n",
    "    else:\n",
    "        final_data_table_for_step5 = master_df_step4[final_table_cols_step5_existing].copy()\n",
    "        \n",
    "        # Add 'UserSessionName' which was previously added in Step 3 Cell 8.\n",
    "        # Here, we re-affirm it as the patient_hemisphere_id for this file.\n",
    "        if 'UserSessionName' not in final_data_table_for_step5.columns:\n",
    "            final_data_table_for_step5.insert(0, 'UserSessionName', patient_hemisphere_id)\n",
    "        else: # If it was somehow carried over from a loaded file that already had it\n",
    "            final_data_table_for_step5['UserSessionName'] = patient_hemisphere_id\n",
    "\n",
    "\n",
    "        # Optional: Sort the table\n",
    "        sort_by_cols_step5 = ['UserSessionName', 'Aligned_PKG_UnixTimestamp', CHANNEL_DISPLAY_COL, FOOOF_FREQ_BAND_COL]\n",
    "        sort_by_cols_step5_existing = [col for col in sort_by_cols_step5 if col in final_data_table_for_step5.columns]\n",
    "        if sort_by_cols_step5_existing:\n",
    "            final_data_table_for_step5.sort_values(by=sort_by_cols_step5_existing, inplace=True, ignore_index=True)\n",
    "\n",
    "        print(f\"  Final data table for Step 5 created with {final_data_table_for_step5.shape[0]} rows and {final_data_table_for_step5.shape[1]} columns.\")\n",
    "        print(f\"  Columns included: {final_data_table_for_step5.columns.tolist()}\")\n",
    "\n",
    "    # Define filename and save (this output path should ideally be outside the patient-specific plot folder,\n",
    "    # in a place where Step 5 can glob all such files)\n",
    "    # The original Step 4 saved this in analysis_plots_root_folder (one level up from session_plot_folder_name_step4)\n",
    "    \n",
    "    output_filename_for_step5 = f\"{patient_hemisphere_id}_CrossSubjectAnalysis_DataTable_{current_datetime_str_step4}.csv\"\n",
    "    # Save in the root of the Step 4 analysis folder (step4_analysis_root_folder)\n",
    "    # This aligns with where Step 5 would look for inputs from multiple subjects.\n",
    "    output_path_for_step5 = os.path.join(step4_analysis_root_folder, output_filename_for_step5)\n",
    "\n",
    "    try:\n",
    "        final_data_table_for_step5.to_csv(output_path_for_step5, index=False)\n",
    "        print(f\"  Successfully saved final data table for Step 5 input to: {output_path_for_step5}\")\n",
    "        print(\"\\n  Sample of this final data table (first 5 rows):\")\n",
    "        print(final_data_table_for_step5.head())\n",
    "    except Exception as e_save_final_step4:\n",
    "        print(f\"  ERROR saving the final data table for Step 5 input: {e_save_final_step4}\")\n",
    "\n",
    "print(f\"\\n--- Cell 11: Final Data Table generation for {patient_hemisphere_id} complete ---\")\n",
    "print(f\"\\n--- All Step 4 processing for {patient_hemisphere_id} complete. Outputs are in {analysis_session_plot_folder_step4} and {step4_analysis_root_folder} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96759d89-ec2c-4c98-b689-54b49f98877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Cell 12 (New): Generate CURATED Final Data Table for Cross-Subject Analysis ---\n",
    "# This cell uses the selections made in Cell 8 to filter the master data table,\n",
    "# adds a new 'BinaryChannel' column (STN/M1), and saves the result to a new CSV file.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"\\n--- Cell 12: Preparing CURATED Data Table for Step 5 (Cross-Subject Analysis) ---\")\n",
    "\n",
    "# --- Prerequisite Checks ---\n",
    "# This cell is entirely dependent on the interactive selections made in Cell 8.\n",
    "if 'user_selections' not in locals() or not isinstance(user_selections, dict) or not user_selections:\n",
    "    print(\"\\nERROR: The 'user_selections' dictionary from Cell 8 was not found or is empty.\")\n",
    "    print(\"Please run Cell 8 to make your interactive channel selections before running this cell.\")\n",
    "\n",
    "elif 'master_df_step4' not in locals() or master_df_step4.empty:\n",
    "    print(\"\\nERROR: master_df_step4 not available or empty. Cannot generate final table.\")\n",
    "\n",
    "else:\n",
    "    # --- 1. Build the Curated DataFrame based on User Selections ---\n",
    "    print(\"   Building curated data table based on selections from Cell 8...\")\n",
    "    curated_data_rows = []\n",
    "    \n",
    "    # Loop through the nested dictionary of user choices\n",
    "    for symptom_dv, choices in user_selections.items():\n",
    "        for group_name, chosen_channel in choices.items():\n",
    "            \n",
    "            # Filter the main dataframe to get all timepoints for the chosen channel\n",
    "            # Note: We filter only by channel here, not by symptom, to get all data for that channel.\n",
    "            # The symptom-specific choice was just to select which channel represents the group.\n",
    "            channel_data_subset = master_df_step4[master_df_step4[CHANNEL_DISPLAY_COL] == chosen_channel].copy()\n",
    "            \n",
    "            # Add the new 'BinaryChannel' column to identify the group (STN or M1)\n",
    "            if not channel_data_subset.empty:\n",
    "                channel_data_subset['BinaryChannel'] = group_name\n",
    "                curated_data_rows.append(channel_data_subset)\n",
    "\n",
    "    if not curated_data_rows:\n",
    "        print(\"   ERROR: Could not build curated data. No matching data found for your selections.\")\n",
    "    else:\n",
    "        # Combine the subsets for each chosen channel into one dataframe\n",
    "        # Use drop_duplicates to handle cases where the same channel was chosen for multiple symptoms\n",
    "        df_curated = pd.concat(curated_data_rows).drop_duplicates().reset_index(drop=True)\n",
    "        print(f\"   Successfully built a curated dataframe with {df_curated.shape[0]} rows.\")\n",
    "\n",
    "        # --- 2. Select and Format Columns for the Final Table ---\n",
    "        # This logic is borrowed from the original Cell 11\n",
    "        \n",
    "        # Add our new column to the list of columns to keep\n",
    "        intended_step5_cols = [\n",
    "            'UserSessionName', 'SessionID', 'Hemisphere', \n",
    "            'BinaryChannel', # Our new curated channel group\n",
    "            CHANNEL_DISPLAY_COL, # Keep original channel name for reference\n",
    "            'Aligned_PKG_UnixTimestamp', 'Aligned_PKG_DateTime_Str',\n",
    "            CLINICAL_STATE_COL, CLINICAL_STATE_AGGREGATED_COL,\n",
    "            'Aligned_BK', 'Aligned_DK', 'Aligned_Tremor_Score',\n",
    "            'Total_Daily_LEDD_mg',\n",
    "            'Beta_Peak_Power_at_DominantFreq', 'Gamma_Peak_Power_at_DominantFreq',\n",
    "            FOOOF_FREQ_BAND_COL, 'BestModel_AperiodicMode',\n",
    "            'Offset_BestModel', 'Exponent_BestModel',\n",
    "            'R2_BestModel', 'Error_BestModel'\n",
    "        ]\n",
    "        \n",
    "        final_table_cols_existing = [col for col in intended_step5_cols if col in df_curated.columns]\n",
    "        \n",
    "        df_final_curated = df_curated[final_table_cols_existing].copy()\n",
    "        \n",
    "        # Ensure UserSessionName is present and correct\n",
    "        if 'UserSessionName' not in df_final_curated.columns:\n",
    "            df_final_curated.insert(0, 'UserSessionName', patient_hemisphere_id)\n",
    "        else:\n",
    "            df_final_curated['UserSessionName'] = patient_hemisphere_id\n",
    "\n",
    "        # Sort the final table for consistency\n",
    "        sort_by_cols = ['UserSessionName', 'BinaryChannel', 'Aligned_PKG_UnixTimestamp', FOOOF_FREQ_BAND_COL]\n",
    "        sort_by_cols_existing = [col for col in sort_by_cols if col in df_final_curated.columns]\n",
    "        if sort_by_cols_existing:\n",
    "            df_final_curated.sort_values(by=sort_by_cols_existing, inplace=True, ignore_index=True)\n",
    "\n",
    "        print(f\"   Final curated data table created with {df_final_curated.shape[0]} rows and {df_final_curated.shape[1]} columns.\")\n",
    "\n",
    "        # --- 3. Save the Curated Data Table to a New CSV File ---\n",
    "        output_filename_curated = f\"{patient_hemisphere_id}_Curated_CrossSubject_DataTable_{current_datetime_str_step4}.csv\"\n",
    "        \n",
    "        # Save in the same root folder as the Cell 11 output for easy access in Step 5\n",
    "        output_path_curated = os.path.join(step4_analysis_root_folder, output_filename_curated)\n",
    "\n",
    "        try:\n",
    "            df_final_curated.to_csv(output_path_curated, index=False)\n",
    "            print(f\"\\n   Successfully saved CURATED data table for Step 5 input to: {output_path_curated}\")\n",
    "            print(\"\\n   Sample of this curated data table (first 5 rows):\")\n",
    "            print(df_final_curated.head())\n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR saving the curated data table: {e}\")\n",
    "\n",
    "print(f\"\\n--- Cell 12: CURATED Final Data Table generation for {patient_hemisphere_id} complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8aaea-9318-4a7a-903c-3ae6c4038e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fooof_env",
   "language": "python",
   "name": "fooof_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
